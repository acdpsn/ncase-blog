---
title: "Clearing out the idea closet (47 projects on my backlog)"
tags:
- post
share_image: https://blog.ncase.me/content/media/backlog/banner.png
share_desc: "47 things I didn't ship."
date: 2024-04-21
readtime: "90 min"
layout: base-layout.njk
hasMath: true
---

![Nicky-Catgirl sweeping lightbulbs off the floor](/content/media/backlog/banner.png)

Wow, I haven't shipped anything in a year and a half, huh? Not even a blog post?

Sure, I've generated lots of *ideas*, at different stages of development, but in the end, all that matters is what's out there. So this month, I'm shipping *this* blog post about all the projects I haven't shipped (yet).

Motivations for this post:

1. To scratch an itch.
2. To give y'all a look into the "creative" "process".
3. To say "First!!1!", in case someone else beats me to an idea. This is petty, and all my work is [Creative Commons](https://creativecommons.org/publicdomain/zero/1.0/deed.en) anyway, but... well, heart wants what the heart wants. And the heart wants to be petty.
4. **To get feedback early & often. Which projects resonate most with _you?_ What would _you_ personally benefit from? This may help me prioritize what to make, or generate new ideas!**

This post contains 47 "project pitches", with the following info:

* The working title
* A ~2 minute pitch
* How much progress made.
* How long it's been on the backburner.
* How much time to finish it. (VERY CRAPPY ESTIMATES)
* How much I *want* to finish it. (So you can set your expectations for *if* I ever ship it, or figure out if *you'd* like to beat me to it.)

![meme of a guy labelled "writer/artist", carrying a zillion things labelled "WIP"](/content/media/backlog/wip.jpg)

**_YOU CAN READ ANY OF THE PROJECT PITCHES IN ANY ORDER, OR SKIP THEM._** Holy moly this full post is over an *hour* long. Each pitch is only a 1 to 5 minute read.

So, without further self-deprecation, here's 47 Things I Have Not Shipped But Might Eventually, Maybe:

---

## Table of Contents

Categorized by interest/topic, but can be read (or skipped) in any order!

If you're on a desktop browser, you can click this icon on the right to access the Table of Contents at any time: &rarr;

![Icon of "table of contents" in the sidebar. Though I guess if you're hearing this alt-text through a screen reader, that'll be of jack diddly use to you.](/content/media/backlog/17084644320266.jpg)

Click ↪ to skip to that project, if it tickles your fancy:

🤔 **[Research Projects](#category_research)**

1. Why all the trans furry programmers? [↪](#project_1)
2. Backwards Logic: logic gates that go forwards & backwards & sideways [↪](#project_2)
3. The Quantum Octopus [↪](#project_3)
4. Beating Goodhart's Law with CLAP: composite limited-award proxies [↪](#project_4)
5. Bio-plausible neural networks [↪](#project_5)
6. Automatically getting causation from correlation [↪](#project_6)
7. Smooth Criminal: exploring the continuous prisoner's dilemma [↪](#project_7)

🤖 **[Explaining AI + AI Safety](#category_ai)**

8. AI Safety for Fleshy Humans [↪](#project_8)
9. You Played Yourself: the game theory of self-modification [↪](#project_9)
10. Will self-improving AI take off fast or slow? [↪](#project_10)
11. What if System 2 just *is* System 1? [↪](#project_11)

❎ **[Explaining Math](#category_math)**

12. Math for people who have been scarred for life by math: or, Algebra in Pictures [↪](#project_12)
13. Why don't brackets matter in multiplication? (novel visual proof!) [↪](#project_13)
14. ...999.999... = 0 (a math shitpost) [↪](#project_14)
15. Gödel's Proof, fully explained from scratch [↪](#project_15)
16. "This sentence is false" is half-true, half-false: Fuzzy Logic vs Paradoxes [↪](#project_16)
17. Your choosing-algorithm should choose itself: intro to Functional Decision Theory [↪](#project_17)

🔔 **[Explaining Science & Statistics](#category_science)**

18. Correlation is evidence of causation, & other Bayesian fun facts [↪](#project_18)
19. Statistics without the Sadistics [↪](#project_19)
20. Causal Networks tutorial [↪](#project_20)
21. Systems Biology tutorial [↪](#project_21)
22. Emotions in 3D: The Valence-Arousal-Dominance Model [↪](#project_22)

🧐 **[Personal Reflections / Philosophy](#category_philosophy)**

23. Emotions are for motions: Notes on using feelings well [↪](#project_23)
24. You don't need faith in humanity for a moral or meaningful life [↪](#project_24)
25. You don't want assholes on your side [↪](#project_25)
26. Virtue Ethics for Queer Atheist Nerds [↪](#project_26)
27. If classic computers can be conscious, then dust is conscious [↪](#project_27)
28. On non-reproductive, similar-in-age, consensual-adult incest [↪](#project_28)

👁‍🗨 **[Stories](#category_stories)**

29. The Lolita Meets Clockwork Orange Story [↪](#project_29)
30. Complex Jane: a love story with an imaginary friend [↪](#project_30)
31. I've Become My Mother: a sci-fi story about raising yourself [↪](#project_31)
32. Genetically Engineered Catgirl [↪](#project_32)

👩‍🎤 **[Songs / Music Videos](#category_music)**

**Turn on your speakers, there's audio of my amateur singing in this one! 🔊**

33. The Special Relativity Song [↪](#project_33)
34. The General Relativity Song [↪](#project_34)
35. The Intrusive Thoughts Song [↪](#project_35)
36. The Pythagorean Theorem Song [↪](#project_36)
37. The Longest Term [↪](#project_37)
38. The 3Blue1Brown Jingle [↪](#project_38)
39. Brand New Fetish [↪](#project_39)

🤷🏻‍♀️ **[Miscellaneous](#category_misc)**

40. Trans voice tools/tutorials [↪](#project_40)
41. Tool for making drip-feed courses (& courses I want to make) [↪](#project_41)
42. (Something on Georgist Economics) [↪](#project_42)
43. (Something on Pandemic Preparedness) [↪](#project_43)
44. The Exit Strategy [↪](#project_44)
45. We do advocacy, they do propaganda: A Collection of Russell Conjugations [↪](#project_45)
46. Common Wisdom is a Two-Timing Sumnabitch [↪](#project_46)
47. Clearing out the idea closet (47 projects on my backlog) [↪](#project_47)

---

<a id="category_research"></a>

## 🤔 Research Projects

After a decade of *explaining* science, I'd like to finally try *doing* science. (well, and math research, if you count math as "science")

Again, just dumping it all here to pettily claim "First!!1!":

1. Why all the trans furry programmers? [↪](#project_1)
2. Backwards Logic: logic gates that go forwards & backwards & sideways [↪](#project_2)
3. The Quantum Octopus [↪](#project_3)
4. Beating Goodhart's Law with CLAP: composite limited-award proxies [↪](#project_4)
5. Bio-plausible neural networks [↪](#project_5)
6. Automatically getting causation from correlation [↪](#project_6)
7. Smooth Criminal: exploring the continuous prisoner's dilemma [↪](#project_7)

. . .

<a id="project_1"></a>

### 1. 😽 Why all the trans furry programmers?

Let's start with a goofy research project idea!

Context: I'm a trans woman, a member of the furry fandom, and a programmer. There is a very, *very* widely-recognized correlation amongst "My People":

![A Venn diagram of "Programmers", "Trans girls", and "Furries", with the center labelled: "A suspiciously large overlap".](/content/media/backlog/17064898642946.jpg)*(from [this popular reddit post](https://www.reddit.com/r/traaaaaaannnnnnnnnns/comments/plxqv1/im_not_the_only_one_whos_noticed_this_right/))*

![Comic. Panel 1: "A group of cats is called a clowder". Panel 2: "A group of catgirls is called a (crossed out) polycule, (scrawled over) dev team."](/content/media/backlog/17064905516761.jpg)*(viral comic, artist unknown? reverse image search did not help.[^no-help])*

[^no-help]: The highest-resolution exact match was [this forum post with no credit](https://knockout.chat/thread/52258/48#post-1916833).

But *does* anecdote match data? If this correlation is real, what causes it? And roughly how many trans furry programmers *are* there in America?

**Progress:** 60% ▶️▶️▶️◻️◻️

* I've collected the stats and confirmed the pairwise correlations are indeed real! Trans folks are ~2x as likely to be Programmers & vice versa; Furries are ~10x as likely to be Programmers & vice versa; Trans folks are *~20x* as likely to be Furries & vice versa.
* Another interesting finding: **in America, there are roughly _as many trans adults as computer programmers_ (~1.6 million)**.
* Doing a Fermi estimate (*Fur*-mi estimate?) using `[ratio of furries who report going to conventions]` & `[number of attendees at the top furry conventions in the U.S.]`, I estimate there's probably `[100,000 to 300,000]` furries in the US.
* I've done a dozen informal interviews with folks who are 2-of-3-of-{trans, furry, programmer}, and consolidated their thoughts.
* I've investigated causal hypotheses, to explain the correlations between {trans, furry, programmer}. (Spoiler: It's probably autism spectrum.)

What's left to do:

* Polish the write-up
* Make a Monte Carlo sim to get error bars & sensitivity to assumptions
* Post a survey in trans programmer subreddits/Discords to ask how many of them also happen to furries, to estimate P(furry | trans & programmer), which will let me calculate... *how many trans furry programmers are there in America?*

**Time on backburner:** 1 year

**Time to finish:** 3 weeks

**Desire to finish:** 4/5 ⭐️⭐️⭐️⭐️☆ (It's cute & close to done anyway)

. . .

Speaking of "probably autism spectrum"...

<a id="project_2"></a>

### 2. ↔️ Backwards Logic: logic gates that go forwards & backwards & sideways

*(I'm sorry the following is very jargon-heavy. But, just clearing out my idea closet....)*

During an isolating 2020 Canadian-winter pandemic lockdown, I went Full Crackpot. I spent four months *obsessed* with (what I thought to be) a promising way to crack the million-dollar P = NP problem.[^PNP]

[^PNP]: [Millennium Prize Problems](https://en.wikipedia.org/wiki/Millennium_Prize_Problems) on Wikipedia. In sum: "P=NP?" is the question, are all problems that are "easy to check" all secretly "easy to solve"?

The inspiration: *A see-saw is a NOT logic gate.* Left side down, right side *up*. Left side up, right side *down*.

![Drawing of a see-saw. When left side is pushed down, right side goes up.](/content/media/backlog/seesaw_1.png)


The important bit: *you can do this in reverse.* Move the *right* side up, and the *left* side goes down.

By manipulating the *output*, you can get the necessary *input*.

![Drawing of a see-saw. When *right* side is pushed down, *left* side goes up.](/content/media/backlog/seesaw_2.png)

So I wondered, was there a way to:

* Build "bi-directional" NOT/AND/OR gates like this,
* Then use them to build a circuit that multiplies two binary numbers
* Then set the *output* of that circuit to some multiple of two primes
* To force the circuit to *automatically give you the two primes as input?*
* And, more generally, could this solve *any* Circuit-SAT problem (an NP-Complete problem)?

(A nuance about bi-directional OR gates; if you set the *output* to 1, it "wouldn't know" whether the inputs should be (0,0), (0,1), or (1,0). But it *can* know it's not (1,1). The hope was that a mechanical circuit [or simulation of it] would constraint-solve that correctly anyway. Same nuance with AND gates.)

Here's a simulation of a fuzzy+reversible Exclusive OR (XOR) circuit. When I pull the output to 1, it constraint-solves the inputs to be 0 & 1, in either order. When I pull the output to 0, it constraint-solves the inputs to be *both* 0 or *both* 1.

![GIF of a simulation of a "fuzzy, reversible circuit"](/content/media/backlog/XOR.gif)

(After independently discovering this idea, I checked to see if it was done before. Of course it was, there's "nothing new under the sun". But, as far as I could find, there was only *one* paper in 2002 that did something similar, with water-based logic gates.[^water-logic] Conceptual, no experiments/simulations done. I couldn't find any "disowning" or "debunking" of their idea, so I guess I just had to *try* it and see if/how it failed.)

[^water-logic]: Arulanandham, Calude, and Dinneen (2002): [Solving SAT with bilateral computing](https://researchspace.auckland.ac.nz/bitstream/handle/2292/3707/199joshua.pdf)

My simulation was messing up for larger cases, and building it for real would take forever. So eventually, all the above led to the following "simpler" algorithm:

* Take a Circuit-SAT problem.
* Replace each logic gate with its fuzzy-logic equivalent: [^fuzzy-operators] \\(NOT(x) = 1-x; \space \space AND(x,y)=min(x,y); \space \space OR(x,y)=max(x,y)\\)
* This gives you a function \\([0,1]^n \rightarrow [0,1]\\) that you can interpret as the "energy landscape" of this mechanical circuit, if \\(output = 1\\) is "down". For example:

[^fuzzy-operators]: These fuzzy gates are the [Zadeh operators](https://en.wikipedia.org/wiki/Fuzzy_logic#Fuzzy_logic_operators) or [Gödel t-norm](https://en.wikipedia.org/wiki/T-norm#Prominent_examples). I *think* they were independently discovered by the mathematicians Lotfi Zadeh & Kurt Gödel, respectively.

![A graph of a logic statement, converted to a fuzzy logic equation.](/content/media/backlog/logic_landscape.png)

* As you can see above, there are lots of zero-gradient "ridges", but you can get rid of them by adding a small "centripetal force" towards the center, where all inputs are 0.5.
* And voilà, you have an energy landscape where *all local minima are global minima, and they're all valid solutions!* There's no flat/vanishing gradients, and there's only *one* saddlepoint, which we know from Deep Learning can be efficiently solved with stochastic gradient descent + momentum! So, I was hoping, that'd be enough to work!...

Alas, it's the most pathological saddlepoint I've ever met, so no dice.

Project pitch: Explain what I found anyway, to save some *other* sucker four months of work if they discover the same idea. Reversible & mechanical logic gates may also be helpful for nanobots or something, I dunno.

**Progress:** 80% ▶️▶️▶️▶️◻️

I got pretty far, as you can see above! "All that's left to do" is polish it up, try a few more ideas, then post it as a writeup.

**Time on backburner:** 4 years

**Time to finish:** 3 weeks

**Desire to finish:** 3/5 ⭐️⭐️⭐️☆☆

. . .

<a id="project_3"></a>

### 3. 🐙 The Quantum Octopus

Okay, let's go a *bit* more crack-potty: maybe the above problem could be solved with quantum physics?

Above, I transformed the NP-Complete Circuit-SAT problem into a gradient descent problem, where all local minima *are* global minima, and there's only one saddlepoint. Unfortunately, that saddlepoint was very pathological. Apparently, this kind of saddlepoint is called an "octopus saddle"[^octopus-saddle].

[^octopus-saddle]: Peckham (2011). [Monkey, starfish and octopus saddles](https://www.researchgate.net/profile/Scott-Peckham-2/publication/256808897_Monkey_Starfish_and_Octopus_Saddles/links/0c960523c9034673f2000000/Monkey-Starfish-and-Octopus-Saddles.pdf)

![Drawing of an "octopus saddle" (looks like a wavy cloth skirt)](/content/media/backlog/octopus_1.png)


(Though, since it's in high n-dimensions, maybe it's a *Cthulhu* saddle?)

Good ol' stochastic gradient descent, even with momentum, can't solve it efficiently since it just brute-forces each "arm" of the octopus:

![Octopus saddle with failed gradient descent: the ball misses the correct arm.](/content/media/backlog/octopus_2.png)


So I wondered: could a quantum wavefunction, because its position is "fuzzy", and thus "feels out" all the arms... and thus pulled by all nearby slopes... could *that* efficiently find its way down an octopus saddle?

![Octopus saddle with a fuzzy blob in the center (representing a quantum wavefunction) correctly finding the correct arm.](/content/media/backlog/octopus_3.png)

(*NOTE: I know quantum computers DO NOT solve problems by just "trying all the solutions", since when you measure it you can't choose which "branch" of the wavefunction it "collapses" into. This idea is different: I wanna see if I can coax ALL of a wavefunction down the correct arm, not just one branch.*)

(Note 2: There are two main kinds of quantum computing. The more famous one is "quantum circuits"; this is the one with the infamous Shor's Algorithm that can break RSA encryption. The other one is "quantum annealing"; this is what D-Wave does. My project idea written above is closer to quantum annealing, though I think it's technically slightly different.)

(Note 3: *The* hard problem in quantum annealing is getting stuck in local minima, especially when the gap between local/global minima gets exponentially small. In the above problem, all local minima *are* global minima, and are valid solutions! And, if one follows my Circuit-SAT-to-landscape conversion, the gap between the saddlepoint and the global minimum stays *constant*. So, if you're a researcher in quantum annealing/adiabatic computation... [email me?](https://ncase.me/faq/contact.jpg))

For more info on quantum annealing, [:click this expandable section](#AdiabaticQuantumComputation)

**Progress: 0%** ◻️◻️◻️◻️◻️

Right now it's just the above idea, but there's "not much" to do:

1. Learn WebGPU, to make physics simulations
2. "Just" simulate the Schrödinger wavefunction on an octopus-saddle-shaped potential well
3. See how many sim-timesteps it takes for most of the wavefunction to go down the correct "arm", as the number of arms increases. If it increases linearly, bummer. But if it's *constant or logarithmic*, we're cooking with *gas!* Not as sexy as P = NP, but BQP = NP[^bqp] ain't bad either!

[^bqp]: The "quantum" version of the P=NP problem: are problems that are "easy to check" secretly all "easy to solve" _with ideal quantum computers?_ (practicalities & decoherence aside)

**Time on backburner:** 3 years

**Time to finish:** 4 weeks

**Desire to finish:** 3/5 ⭐️⭐️⭐️☆☆

(Even if it fails, at least I'd learn WebGPU + making physics sims, which I could publish as a standalone toy.)

. . .

<a id="project_4"></a>

### 4. 👏 Beating Goodhart's Law with CLAP: composite limited-award proxies

The writer who pumps out clickbait to make views go up. The student who uses ChatGPT to make grades go up. The politician who makes false promises, then blames their opponents for they fail, to make votes go up.

*What gets measured gets gamed.* This is (roughly paraphrased) Goodhart's Law.

For the longest time, this law was "just" phrased in words & not formalized. But a few years ago, an acquaintance of mine, Yohan John, showed me his paper where he analyzes Goodhart's Law with causal modeling![^john] (Manheim & Garrabrant also did it in another paper[^causal-2], but as far as I know these were independently discovered.)

[^john]: John et al (2023): [Dead rats, dopamine, performance metrics, and peacock tails: proxy failure is an inherent risk in goal-oriented systems](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/dead-rats-dopamine-performance-metrics-and-peacock-tails-proxy-failure-is-an-inherent-risk-in-goaloriented-systems/89408A43F6D14BFD368FE5225A573032)

[^causal-2]: Manheim & Garrabrant (2019): [Categorizing Variants of Goodhart’s Law](https://arxiv.org/pdf/1803.04585.pdf)

Here's the clickbait problem, as a causal diagram ⤵ (Read: "quality" and "clickbait" both affect "views", but clickbait has twice the effect.)

![A circles-and-arrows network diagram. "Quality" and "clickbait" both point to "views". The arrow from "clickbait" to "views" is labelled "2x".](/content/media/backlog/causal_1.png)


Let's re-write that as an equation:

\\( views = quality + 2\times clickbait \\)

Clickbait has 2x the Return-on-Investment (RoI) on Views than Quality. Naïvely, I thought this meant to maximize views, you'd put 2x more energy into Clickbait than Quality. Actually, it's worse: you'd put *all* your energy into Clickbait, not Quality.

Same problem for any kind of proxy-measure: there's almost always *some* other higher-RoI way to game it:

![Causal Network Diagram. "Our desired target" affects 3 Proxies, but all 3 Proxies are twice as affected by Noise.](/content/media/backlog/causal_2.png)


As equations:

\\( proxy_1 = Target + 2\cdot noise_1 \\)    
\\( proxy_2 = Target + 2\cdot noise_2 \\)    
\\( proxy_3 = Target + 2\cdot noise_3 \\)    

So, what to do? Answer: *add up all the proxies!*

\\( composite = 3\cdot Target + 2 \cdot noise_1 + 2 \cdot noise_2 + 2 \cdot noise_3 \\)

By adding the proxies together, our desired Target's coefficient outweighs all the Noise coefficients; hence, *the Target* now has the highest RoI!

But what if a Noise coefficient is *much* higher? That's my second idea: to prevent a few unusually-high Noise coefficients from wrecking everything, *cap the maximum contribution from each proxy*.

\\(composite = min(proxy_1,1) + min(proxy_2,1) + min(proxy_3,1) + ...\\)

Hence, **CLAP: Composite Limited-Award Proxies.** Yes I'm very proud of that acronym.

Other research connections/directions:

* It's been known for a long time that Simple Prediction Rules (SPR) like the above are *better than human experts* at predicting stuff.[^spr] But it's an open question if SPRs can be used for *control* purposes, like I'm suggesting above, while avoiding Goodhart's Law.
* Can CLAP be applied to neural networks (cap the *maximum magnitude* of each weight) to "smooth out" the function the network approximates, to become robust to Adversarial Examples? e.g. Google's AI thinking a toy turtle is a gun[^adversarial], by just adding a few dots. (It's already known that *penalizing* large weights ["L2 regularization"] in neural networks helps with overfitting & adversarial examples[^l2]; I wonder if *straight-up capping them* would help more?)

[^spr]: Bishop & Trout (2005) give a snappy overview: [The Amazing Success of Statistical Prediction Rules](https://academic.oup.com/book/36404/chapter-abstract/320070686?redirectedFrom=fulltext) ([pdf](https://joelvelasco.net/teaching/4330/bishop&trout-ch2.pdf))

[^adversarial]: On YouTube: [Fooling Image Recognition with Adversarial Examples](https://www.youtube.com/watch?v=piYnd_wYlT8) (2017)

[^l2]: See the final figure in the "Example: LeNet on MNIST" section of [Tanay & Griffin (2018)](https://thomas-tanay.github.io/post--L2-regularization/). The middle row are the adversarial examples (AE). On the left, with *low* L2 regularization, the AE's are basically indistinguishable to the human eye. On the right, with *high* L2 regularization, the AEs are clearly manipulated & messed up; hence, a human would pick up on those errors better.

**Progress: 40%** ▶️▶️◻️◻️◻️

So far, I've made an "AI" to game given reward metrics (it's just simulated annealing), and confirmed that CLAP works! (so far)

What's left to do: more sims, with randomly-generated causal networks, then polish it with a write-up.

**Time on backburner:** 2 years.

**Time to finish:** 2 weeks.

**Desire to finish: 4/5** ⭐️⭐️⭐️⭐️☆ (I also *especially* want to try the CLAP neural network. That seems like a promising route to robustness in AI Safety!)

. . .

Speaking of neural networks...

<a id="project_5"></a>

### 5. 🧠 Bio-plausible neural networks

Another research direction I want to explore is: *copying evolution's homework!*

That strategy's worked so far: Artificial neural networks (ANNs) are inspired by brains. Convolutional neural networks (CNNs) for machine vision are inspired by the mammalian visual cortex. Could copy-biology *also* work for the current hurdles in AI?

4 specific problem-experiment areas:

*(Again, I'm sorry the following is VERY jargon-heavy. Clearing out idea closet...)*

* Problem: ANNs are fragile (see: turtle-gun[^adversarial]). Solution? Giving ANNs a "small-world topology" (like real brains have) increases robustness![^small-world] Unknown: are small-world ANNs *also* easier to mechanistically interpret?
* Problem: Vanishing/exploding gradients make it hard to train deep networks, *especially* "recurrent" networks (networks with "feedback loops"). Solution? Find a more efficient alternative to "backpropagation" (standard way of training ANNs), inspired by biology? Keywords: integrate-and-fire, Hebbian learning, BCM, STDP, eligibility traces.
* Problem: Finding "hyperparameters" for an ANN is annoying & hard. Solution? Would adding "heterogeneity" to the neurons (i.e. each one has a different learning rate) make a network *automatically* find a good architecture? Heterogeneity comes for free in biology, due to noise!
* Problem: Data/vectors need to be regularized & normalized. Solution? Simulate "homeostasis", so the normalization is built *into* the neurons, not some external process?

(Note of caution: the above research may accelerate AI *Capabilities* more than AI Safety, even if they *do* help with robustness & interpretability.)

**Progress:** 0% ◻️◻️◻️◻️◻️ lol none

**Time on backburner:** 2 years.

**Time to finish:** 6 months??? 1.5 months for *each* of the above 4 problem-experiment areas.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆ Seems hard, and risky from an AI Safety view.

. . .

Speaking of AI Safety...

<a id="project_6"></a>

### 6. 🤖 Automatically getting causation from correlation

One approach to AI Safety is "don't build the torture nexus". Just get the *upsides* of AI without risking the *downsides*, by making AI that does one thing well. (Why would your self-driving car *need* meta-cognition?)

What's in the Venn Diagram intersection of:

![Venn Diagram of "High upside: huge benefits if automated" and "Low downside: doesn't require new advances in AI", with a big question mark in the middle.](/content/media/backlog/venn.png)

One idea: *Causal inference – extracting causation from correlation, the right way!* This could be huge for public health, economics, policy, etc; every field where it'd be impractical or unethical to do experiments on humans.

Here's three *life-saving* examples of causal inference:

1. John Snow looked at which households got cholera, and discovered that the standard miasma theory (disease passes through bad smells) was wrong. Instead, cholera spread through the *water supply.*
2. Researchers in the 1950's discovered that smoking was a large risk factor for lung cancer. Without needing to, y'know, force an experimental control group to smoke for decades then wait for them to die of cancer.
3. (More recent & controversial) Jean Twenge & Jon Haidt testing & eliminating every other plausible hypothesis to show that, no really, it *is* social media & smartphones causing the recent spike in depression & suicide in the Anglosphere.[^haidt-twenge] Now even the Surgeon General warns of it, too![^surgeon-general] (NOTE: it only explains the *recent* spike. The previous "baseline level" of youth suicide has no consensus explanation yet.)

[^haidt-twenge]: Twenge (2023): [Here are 13 Other Explanations for the Adolescent Mental Health Crisis. None of Them Work.](https://www.afterbabel.com/p/13-explanations-mental-health-crisis)

[^surgeon-general]: From HHS.gov, May 2023: [Surgeon General Issues New Advisory About Effects Social Media Use Has on Youth Mental Health](https://www.hhs.gov/about/news/2023/05/23/surgeon-general-issues-new-advisory-about-effects-social-media-use-has-youth-mental-health.html)

What if *this kind of discovery* could be fully automated?

Standard statistics on Big Data isn't enough, because those can only do *correlations*, not infer causations. You *do* need some "common sense" to do causal inference, or at least to filter out the spurious/uninteresting stuff.

*But that's where large language models (LLMs) can help!* Since they're trained on Wikipedia etc, you can use them as a "common sense module"! For example, if an AI saw a correlation between rainy days & car accidents, a "common sense module" could rule out car accidents *causing* rainy days.

(Of course, LLMs hallucinate, so you still need a human as a final-pass filter. But as long as the *first*-pass filter has more signal than noise, *automatic causal inference* could be huge for science!)

It's unethical to do *experiments* on unwilling humans... but the "real world" *already* does experiments on us without our consent; we might as well grab the data & figure out their causes, to improve our lives.

**Progress:** 5% ▶️◻️◻️◻️◻️

Some proof-of-concepts within ChatGPT, but the next step is to learn OpenAI's API, then try out all the standard causal inference techniques[^mixtape] using their LLM as a "common sense module" on, say, a data-dump from Our World In Data.

[^mixtape]: I'm gonna read this book by Scott Cunningham. It's free! [Causal Inference: The Mixtape](https://mixtape.scunning.com/)

**Time on backburner:** 3 months

**Time to finish:** 2 months

**Desire to finish:** 4/5 ⭐️⭐️⭐️⭐️☆ Seems (relatively) simple, but high-possible-reward for both science *and* AI Safety!

. . .

<a id="project_7"></a>

### 7. 🕺 Smooth Criminal: exploring the continuous prisoner's dilemma

Get it? *Continuous* version of the *prisoner's* dilemma? *Smooth Criminal?*

![A photo of a slack-jawed moray, looking like it just told a bad pun.](/content/media/backlog/17064737174567.jpg)

Anyway: I want to replicate Axelrod's famous Iterated Prisoner's Dilemma (IPD) tournament, but with *continuous* payoffs & strategies. The game: Choose how much between \\(x = [0,1]\\) to give, the other player gets \\(3x\\) as payoff, and vice versa.

The winner of Axelrod's original tournament was the simple "tit for tat": start with full cooperation, then copy the other player's move from then on. But I wonder, with *continuous* strategies, would it make more sense to start with *zero* cooperation, then copy the other player's move *plus a bit extra?* This allows for a *gradual building* of trust, but won't get suckers, *and* can recover from mistakes!

(I also wonder how the *other* classic IPD strategies, like Pavlov, could be adapted to a "continuous" world. And beyond the classics, what *other* strategies might be possible?)

**Progress:** 0% ◻️◻️◻️◻️◻️ None. Just a pun & a thought.

**Time on backburner:** 5 years

**Time to finish:** 4 weeks

**Desire to finish:** 1/5 ⭐️☆☆☆☆ Eh, nice to have, but no burning desire.

---

<a id="category_ai"></a>

## 🤖 Explaining AI + AI Safety

Beep boop.

8. AI Safety for Fleshy Humans [↪](#project_8)
9. You Played Yourself: the game theory of self-modification [↪](#project_9)
10. Will self-improving AI take off fast or slow? [↪](#project_10)
11. What if System 2 just *is* System 1? [↪](#project_11)

. . .

<a id="project_8"></a>

### 8. 😼 AI Safety for Fleshy Humans

Okay this one's not on my "backlog", it's *supposed* to be my main project. But since I've been chipping away at this for *14 months*... well, it sure doesn't feel like it's *not* on my backlog.

Anyway, the pitch:

> The AI debate is actually 100 debates in a trenchcoat.
>
> Will artificial intelligence (AI) help us discover cures to all disease, and build a post-scarcity world full of flourishing lives? Or will AI help tyrants surveil, oppress, and manipulate us further? Are the main risks of AI from accidents, abuse from bad actors, or a rogue AI _itself_ being a bad actor? Is this all just a hype bubble? Why can AI imitate any artist's style in a minute, yet gets confused drawing more than 3 objects? Why is it hard to make AI robustly serve humane values, or robustly serve _any_ goal? What if an AI learns to be _more_ humane than us? What if an AI learns humanity's _inhumanity_, our prejudices and cruelty? Are we headed for utopia, dystopia, extinction, a fate _worse_ than extinction, or — the most shocking outcome of all — _nothing changes?_ Also: will the AI take my job?
>
> ...and many, many more questions.
>
> Alas, to understand AI with nuance, we need to understand lots of technical detail... but that detail is scattered across hundreds of articles, and buried six-feet-deep in jargon.
>
> Hence, I present to you:

![Banner image reading: "A SWEEPING TOUR GUIDE of AI SAFETY for US WARM, NORMAL FLESHY HUMANS". Below, a robot cat boy with a sign saying "100% human".](/content/media/backlog/image%20-1-.png)

> **This 3-part series is your one-stop-shop to understand the core ideas of AI & AI Safety, explained in a friendly, accessible way!**
>
> It also has comics starring a Robot Catboy Maid. Like this one:

![Comic. A "human" cat and a robot cat talk. The human says: Keep this house clean! The robot thinks... Objective: Keep house clean. Question: What causes the mess? Answer: The humans cause the mess! Therefore: (scary picture) GET RID OF THE HUMANS. Robot then unceremoniously "yeets" the human out of the house.](/content/media/backlog/Outer_Alignment.png)

(I posted these comics as a sneak-peek on Patreon, and they went viral on reddit with 15k+ upvotes... but my series wasn't out in time to cash in on that virality 😭)

**Progress:**

Part One: 85% ▶️▶️▶️▶️◻️ (Penultimate draft done, ~9,000 words)
Part Two: 60% ▶️▶️▶️◻️◻️ (First draft done, ~10,000 words)
Part Three: 20% ▶️◻️◻️◻️◻️ (All research & main outline done, plus some art)

**Time on backburner:** 14 months

**Time to finish:**

Part One: 2 weeks
Part Two: 3 weeks
Part Three: 4 weeks

**Desire to finish:** 3/5 ⭐️⭐️⭐️☆☆  Not gonna lie, I'm pretty burnt out on this, but I just *have to* finish it. I *do* care about the topic, and I've been promising it for over a year... it's just taken *so dang long. Gahhhhhhhhh.*

. . .

<a id="project_9"></a>

### 9. 🪞 You Played Yourself: the game theory of self-modification

You ever place your smartphone *across* the room before you sleep, instead of next to your bed... so that when the alarm goes off, you're forced to get up to turn it off, & not be tempted to browse memes in bed?

(Or some other "tricking your future self" thing?)

Congratulations, you just broke a fundamental assumption of standard game theory, which is the basis of modern economics, political science, and Artificial Intelligence!

That assumption is *we have preferences in some set order.* But the above smartphone alarm example *isn't* explainable with a stable preference-order:

* If you prefer `browsing memes > getting up`, you wouldn't put your phone across the room, you'd put it next to your bed to browse memes.
* If you prefer `getting up > browsing memes`, you wouldn't *need* to put your phone across the room, you'd just get up when it's time to get up.

The only way this is explainable – and it's how we intuitively think about it anyway – is like *you're playing a game against your future self.*

Here's the game's choice-tree:

![Decision-tree visualizing above main text.](/content/media/backlog/choice_tree.png)

The project: A distillation of the very-little research so far on self-modifying game theory! And hopefully, adding some of my *own* original(?), helpful(?) research!

A "game theory of self-modification" would have lots of applications to human behaviour & AI Safety:

Humans:
* Given that we *can't* be "ideal rational", and *have* to be "bounded rational", what "meta-rational" tricks help work around our bounded rationality?
* Virtue ethics & the Serenity Prayer, re-derived from game theory?

AI Safety:
* Why a self-modifying AI _won't_ just hack its own brain to get maximum reward (wireheading).
* Explaining Stuart Russell's proposal for AI Safety: the AI's only reward is to maximize the *human's* reward, but the AI *is uncertain about it* and *knows* it's uncertain about it.
* Original(?) research showing that Russell's proposal is robust to "bounded rationality" *and* self-modification!
* Original(?) research showing that a "meta-rational" AI using Russell's proposal -- contrary to the instrumental convergence/AI basic drives paradigm -- would choose to *NOT* seek power, beyond the point it can be *sure* it won't be corrupted. (Throw the One Ring into Mount Doom)

Usually, AI game theory is explained with dense math notation. At first I thought, "Why don't I explain it with readable pseudo-code"? Then I realized... Wait, why don't I just write *actual code*, that readers can play with in the browser, to try their *own* game-theory experiments? And so that's what I'm doing!

Code of the Smartphone Alarm problem:

![Screenshot of code](/content/media/backlog/selfmod1.png)

The thought-tree in produces:

![Screenshot of output](/content/media/backlog/selfmod2.png)

In progress: Visualizing thought-trees as procedurally generated trees. (It's bouncy because I'm using spring-physics to figure out how the tree should be spaced out![^force-directed])

[^force-directed]: It's the [force-directed graph drawing](https://en.wikipedia.org/wiki/Force-directed_graph_drawing) algorithm, but with the vertical (y) positions locked.

![tree_viz](/content/media/backlog/tree_viz.gif)


**Progress:** 20% ▶️◻️◻️◻️◻️

The core code & visualization is almost done! Left to do: catch up on the (very small) research literature, do more of my own experiments, then write it up & post it with editable, interactive simulations!

**Time on backburner:** 1 month

**Time to finish:** 1 month

**Desire to finish:** 5/5 ⭐️⭐️⭐️⭐️⭐️ Fun code & visuals, popularizing lesser-known beautiful ideas, *and* (maybe) creating useful new research for game theory, human psychology, and AI Safety!

. . .

Speaking of AIs that can self-modify...

<a id="project_10"></a>

### 10. 🧨 Will self-improving AI take off fast or slow?

First off: even though I think the "Fast Takeoff" or *"FOOM!"* argument is wrong, I appreciate it even *can* be wrong, because its proposers formalized it in a precise way. Too many hypotheses use weasel words that "aren't even wrong".

To summarize (one thread of) the debate on "AI Takeoff Speeds":

Yudkowsky and Bostrom argued that self-modifying AI would go (practically) infinite in a finite amount of time -- this is the original & mathematical definition of a *Singularity*. (coined by Vernor Vinge) Their argument, formalized:

* Let \\(C\\) be an AI's "level of software-engineering [C]apability".
* Let \\(C'\\) be the change/growth in that Capability over time.
* How fast is that growth, if an AI is improving itself? Not *only* should it depend on \\(C\\) itself, because software engineering is its Capability, it will depend on \\(C\\) *again*, because "the optimizer is itself being optimized".
* So, we get:

$$
C' = C \times C = C^2
$$

(I'm leaving out constants for pedagogical reasons, but it doesn't change the results)

If you plot the above on a graph, you get infinite growth in a finite amount of time: (Note: Yudkowsky & Bostrom emphasize there *must* be a theoretical ceiling, but that ceiling is so high it's "practically" infinite.)

![Graph of AI Capabilities over Time, going to infinity in finite time.](/content/media/backlog/foom0001.png)

You may think, hang on, didn't they just double-count \\(C\\) twice? If you dropped that assumption, you get:

$$
C' = C
$$

Which results in "mere" exponential growth. Which, believe it or not, is called a "slow" takeoff in the AI Safety community!

![Graph of AI Capabilities over Time, exponential growth.](/content/media/backlog/foom0002.png)

But, as [Ramez Naam & Paul Baumbart (2014)](http://www.antipope.org/charlie/blog-static/2014/02/why-ais-wont-ascend-in-blink-of-an-eye.html) found, the true AI self-improvement growth would be even slower than *that*, because it doesn't take into account "computational complexity".

For example, if you had a Sudoku game that was \\(N\\) cells wide, then merely *checking* if a solution is correct takes \\(N^2\\) steps. (N-squared, coz Sudoku is a square) *Finding* a solution is even worse: the growth in steps is *exponential*. (well, as far as we know. P=NP is unsolved.)

Let's make the generous assumption that AI self-improvement is only as hard as *checking* a Sudoku solution. Then, dividing potential growth by its difficulty:

$$
C' = \frac{C}{difficultyOfImproving(C)}
$$

And since \\(difficultyOfImproving(C) = C^2\\):

$$
C' = \frac{C}{C^2} = \frac{1}{C}
$$

Which if you plot on a graph, looks like this:

![Graph of AI Capabilities over Time, decelerating/growth slowing down over time.](/content/media/backlog/foom0003.png)

Not infinite-in-finite-time, not exponential, not even linear... it *decelerates* over time.

And if the difficulty of self-improvement is even harder than *merely checking* a Sudoku solution, then the deceleration would be harsher. Paul Baumbart simulated different scenarios for different complexities:

![Graph of AI Capabilities over Time, for various assumptions of how hard AI self-improvement is.](/content/media/backlog/17068066312112.jpg)*(from [this blog post](http://www.antipope.org/charlie/blog-static/2014/02/why-ais-wont-ascend-in-blink-of-an-eye.html))*

Which makes sense, if you think about the *other* self-improving agents we know about: corporations. Yes, in the *beginning* they may grow exponentially, but in the long run there's always diminishing rewards. But the above argument shows it *mathematically:* it's not just a *human* limit, it's a *theoretical limit* for any self-improving agent -- AI, humans, even evolving viruses!

(But the analogy to corporations also makes it clear we shouldn't be complacent: even something that grows "slowly" can still outsized impact on the world.)

Next, Gwern made some counter-arguments against Naam & Baumbart's argument.[^gwern] Some aren't really relevant, some are very valid. (In particular: *yes,* the constants matter, even small advantages matter, and we should address the "empirical scaling laws".)

[^gwern]: Gwern (2014): [Complexity no Bar to AI](https://gwern.net/complexity)

Aaaaand that's the end of this *one* thread on the AI Takeoff debate, so far!

Project Pitch:

* Distill all the arguments, counter-arguments, counter-counter-arguments
* Explain the math of differential equations in a more accessible & visual way than what I wrote above.
* Have editable in-browser simulations a reader can play with, to try different assumptions/scenarios.
* Maybe a cute animation of a marathon-running Robot Catboy, as a metaphor for self-improving skill.

**Progress:** 10% ◻️◻️◻️◻️◻️ Fully outlined, all research done

**Time on backburner:** 2 years

**Time to finish:** 2 weeks

**Desire to finish:** 3/5 ⭐️⭐️⭐️☆☆

. . .

Speaking of thinking fast & slow...

<a id="project_11"></a>

### 11. 👩‍🏫 What if System 2 just *is* System 1?

I'm not sure if this project "wants" to be intellectual self-help (in the style of *How Not To Be Wrong* or *The Scout Mindset* or *Intuition Pumps*), or more of a "AI research directions" thing. Anyway, clearing out the idea closet...

<u>The "Intellectual Self-Help" Side:</u>

Georg Polya's *How To Solve It* was a hit among math students, and rightfully so. But what shocked *me* most about it is that he seems to have... created *formulas* for *creative problem-solving?*

The formulas are "just" a bunch of "when X then Y" habits, like:

* When: Not sure how to even *begin* solving a problem
    * Then: Turn it into a simpler problem, or carve out a smaller sub-problem, then try to solve that.
* When: Finished solving a problem
    * Then: *Exploit your success!* Use what you learnt to try to solve harder or similar problems.

(When-Then's are also called Implementation Intentions or Trigger-Action Plans [TAPs])

In cognitive science, "System 2" deliberate, systematic thinking is considered very different from "System 1" automatic, intuitive thinking. But what if System 2 reasoning just *IS* a lot of System 1 habits?

And if so, *could you install a high-level of critical thinking & rationality into someone*, merely by giving them a bunch of flashcards to ingrain a few dozen When-Then's into them?

And if so, *which* When-Then's?

(And then my project would propose a few dozen Rationality When-Then's, and use [Orbit](https://docs.withorbit.com/) and/or [Anki](https://apps.ankiweb.net/) to give you spaced repetition flashcards.)

<u>Application to AI:</u>

Good Ol' Fashioned AI could beat the world grandmaster at chess, but couldn't recognize pictures of cats. (System 2, no System 1) Modern Deep Learning is great at not just recognizing *and* generating pictures cats, but sucks at baisc logic puzzles. (System 1, no System 2)

You could draw the two tales of AI on a graph like this:

![2D graph. Vertical-axis is "System 1 intuition". Horizontal-axis is "System 2 logic". Good Ol' Fashioned AI from the 1970's/80's had super-human logic, but basically no intuition. Modern Deep Learning AI from the 2010's/20's approached human intuition, but have sub-par logic skills.](/content/media/backlog/dRwUwA9n87XBNxlDkMcpxE1QSXpDLFnflU-_Yc4CeUg5MENgIGvt8m7zoOyamv_VLzC53wS97emVIFluj5ZDbtIyfvCaslV6kpcikwhHFUtymRwT6I84c_5Bt_tU-qoIK-OdgdaXW02MC7bWGyHusvI.png)

One of the founding pioneers of deep learning, Yoshua Bengio, said it explicitly: the next step for AI is to *merge* System 1 & System 2.[^bengio] *That's* how we'd get true "Artificial General Intelligence" (AGI).

[^bengio]: YouTube embed & summary of his 2019 talk [on this blog post](https://bdtechtalks.com/2019/12/23/yoshua-bengio-neurips-2019-deep-learning/).

But if System 2 just *is* System 1... then where's our AGI? What, exactly, is missing?

The times when deep learning AI (like GPT) exhibit the most System 2-like thinking, is when they use their own outputted text as a blackboard/scratchpad, and are *explicitly* told to "think step by step".

I suspect that the [Blackboard](https://en.wikipedia.org/wiki/Blackboard_system)/[Pandemonium](https://en.wikipedia.org/wiki/Pandemonium_architecture) AI Architectures proposed in the mid-1900s are basically correct, now we "just" need to figure out how to train one as a recurrent neural network (RNN).

Unfortunately (or fortunately?) RNNs are notoriously hard to train using the current AI learning methods, so I suspect a new learning algorithm needs to be discovered. (Hence, my above Project Pitch for more bio-inspired AI research.)

**Progress:** 0% ◻️◻️◻️◻️◻️ Bunch of ideas, no idea how to tie them together.

**Time on backburner:** 1 year

**Time to finish:** 3 weeks

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

---

<a id="category_math"></a>

## ❎ Explaining Math

“Georg Cantor stared into the heart of infinity, then died in an asylum. Kurt Gödel shattered the foundations of logic, then starved himself to death. Now, it is our turn to learn mathematics.”[^quote-inspiration]

[^quote-inspiration]: Riff on the opening lines from a physics textbook written by David L. Goodstein: [(content note: suicide)](https://www.snopes.com/fact-check/science-textbook-gloomy-intro/)

12. Math for people who have been scarred for life by math: or, Algebra in Pictures [↪](#project_12)
13. Why don't brackets matter in multiplication? (novel visual proof!) [↪](#project_13)
14. ...999.999... = 0 (a math shitpost) [↪](#project_14)
15. Gödel's Proof, fully explained from scratch [↪](#project_15)
16. "This sentence is false" is half-true, half-false: Fuzzy Logic vs Paradoxes [↪](#project_16)
17. Your choosing-algorithm should choose itself: intro to Functional Decision Theory [↪](#project_17)

. . .

<a id="project_12"></a>

### 12. 😱 Math for people who have been scarred for life by math (or, Algebra in Pictures)

Algebra is the path to the rest of math & science. But for too many folks, instead of being the gateway, it's the gatekeeper.

Anecdotally, algebra is one of *the* most failed courses in college, and where lots of high school students decide they're "not math people". (I couldn't find hard stats dis/confirming the idea that Algebra is the Great Filter, but educators I've asked so far find it plausible.)

Unfortunately, I've never seen a... *good* explainer of K-12 algebra? In contrast, there are *amazing* explanations of more advanced topics — e.g. 3Blue1Brown on calculus and *linear* algebra — but every explanation I've seen of K-12 algebra has 1) no visual intuition, 2) no motivation behind the ideas, and 3) no proof of why things are the way they are, it's all just memorize, plug-and-chug.

(For example, I realized while outlining this project, that I never learned *why* brackets don't matter in multiplication ("associativity"). That is, \\((a \times b) \times c = a \times (b \times c)\\). Yes, it's an "axiom", but why does *this specific axiom hold for multiplying numbers?!* Because associativity *isn't* true for some operators! Like subtraction: \\((3 - 2) - 1 \neq 3 - (2 - 1)\\).)

Anyway, this is why I want to write the K-12 algebra book I wish *I* had.

(Or, if I may be a bit gooey about it, one day I'd like to have a kid, and I want to write a motivated, intuitive, and *beautiful* K-12 algebra book for them.)

**Progress:** ~2% ◻️◻️◻️◻️◻️ An outline, some sketched visualizations.

**Time on backburner:** 2 years.

**Time to finish:** 4 months.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_13"></a>

### 13. ❎ Why don't brackets matter in multiplication? (novel visual proof!)

As mentioned above: to the best of my knowledge, *there does not exist a visual proof* of why the "associativity" axiom, \\((a \times b) \times c = a \times (b \times c)\\), is true for multiplying whole numbers.

There *is* a proof of associativity for multiplying *more* than 3 numbers, but you only see it in college & it still assumes it's an axiom for 3 numbers. Kind of demotivating, isn't it? "Hey kid, memorize this fundamental fact, you'll only learn *why* it's true 10 years later, except not really."

I *was* hoping to save the grand reveal of my (as far as I know) first-ever visual proof of a fundamental axiom, for when I make a video about this...

...but, let's just clear out the idea closet. Here it is.

First, what do brackets mean in math? It means "do this first".

![(I'm sorry, I can't do accessible alt text for the following pictures, because it's a visual proof. The main text should be good enough for a rough mental image, hopefully.)](/content/media/backlog/math0001.png)

Now, how can we visualize multiplication? Remember "*3* times *4*" means "*3* groups of *4*". Like, 3 boxes of 4 balls each:

![](/content/media/backlog/math0002.png)

Then, "doing" a multiplication means *removing the box container:*

![](/content/media/backlog/math0003.png)

Now, let's picture 2 x 3 x 4. That's 2 boxes of 3 boxes of 4 balls each:

![](/content/media/backlog/math0004.png)

Since doing a multiplication means *removing a box container*, it should now be clear: it *does not matter which order you remove the box containers, you'll always end up with the same number of balls.*

![](/content/media/backlog/math0005.png)

In other words, it doesn't matter which multiplications you do first.

In other *other* words, *brackets don't matter* in multiplication!

Best of all, with this visual, we don't *need* the college-level proof: we can instantly see that "associativity" must also be true for multiplying more than 3 numbers!

Here's 2 x 3 x 2 x 3 x 2, bracketed in three different ways, all leading to the same result:

![Animation of "2 x 3 x 2 x 3 x 2" represented as boxes-within-boxes. No matter which way the boxes are removed, the number of balls in the end must stay the same.](/content/media/backlog/animation.gif)

(Granted, this proof only works for *whole* numbers... but still! Kinda shocking a visual proof of such a basic fact *did not* exist before!)

**Progress:** 80%. ▶️▶️▶️▶️◻️

I guess I could just post the above as a standalone blog post right now. **[Actually, fine, _I just did that_.](https://blog.ncase.me/brackets-dont-matter/)**

That's good enough for now; in the future I'd still like to make a video for it.

**Time on backburner:** 2 years.

**Time to finish:** 3 weeks. (to make a video)

**Desire to finish:** 3/5 ⭐️⭐️⭐️☆☆

. . .

<a id="project_14"></a>

### 14. 💩 ...999.999... = 0, a math shitpost

Now that I've told you about two helpful math-explainers, here's one that's _anti_-helpful:

![A "galaxy brain" meme. The small brain says 0.999... = 1. The big brain says ...999 = -1. The galaxy brain says ...999.999... = 0.](/content/media/backlog/spost.png)

This is a video, with three parts:

*  Multiple proofs that 0.999... = 1 (in the real numbers)
*  Multiple proofs that ...99999 = -1 (in the 10-adic numbers)
*  Multiple "proofs" that ...999.999... = 0.

The first two *are* widely-accepted mathematics.

One of many proofs for the first: If you accept that \\(0.333... = 1/3\\), multiply both sides by 3 to get \\(0.999...=1\\). Tada 🎉

One of many proofs for the second: Let's say \\(...9999=x\\). What *is* \\(x\\)? Well, multiply both sides by 10, to get \\(...9990=10x\\). Subtract the second equation from the first, to get \\(9=-9x\\). The only way this is possible is if \\(x=-1\\). Tada 🎉

Then, to "prove" the third, just add \\(0.999...=1\\) and \\(...999=-1\\) to get \\(...999.999... = 0\\). Not convinced? Here's another proof: let's say \\(...999.999... = x\\). Multiply both sides by 10. The right side becomes \\(10x\\). For the left side, that shifts the decimal point one digit to the right, but it's infinite 9s on either side, so it stays the same. So you get: \\(...999.999... = 10x\\). Hence, \\(x = 10x\\). The only way this is possible is if \\(x = 0\\). Tada 🎉

(This math shitpost was inspired by [this Math Overflow discussion](https://math.stackexchange.com/questions/4792771/does-999-999-0) on October 2023, which I found through [Mark Dominus's blog post](https://blog.plover.com/math/se/2023-10.html). Apparently the third thing *is* possible in the "10-adic solenoids" but fuck me if I know what that means.)

**Progress:** 20% ▶️◻️◻️◻️◻️ The above video thumbnail & outline.

**Time on backburner:** 1 week ago.

**Time to finish:** 2 weeks.

**Desire to finish:** 1/5 ⭐️☆☆☆☆ I don't know if this joke is funny enough to spend 2 weeks of my life on it.

. . .

Speaking of weird proofs...

<a id="project_15"></a>

### 15. 🔁 Gödel's Proof, explained 100% from scratch

![The words "Gödel's Proof" but with googly eyes on the O's](/content/media/backlog/godel.png)

Once upon a time, mathematicians wanted to place their field on a solid foundation of logic. Then in 1931, some young punk named Kurt Gödel shattered that dream forever. Then Gödel starved himself to death. The end.

(Happy post-credits scene: Gödel's proof inspired a gay British lad to invent a lil' niche idea you may have heard of called *the computer?!*)

Gödel's Proof is one of the most important proofs in history. There are already multiple (very good!) explanations of it: Derek Muller on [Veritasium](https://www.youtube.com/watch?v=HeQX2HjkcNo), Natalie Wolchover on [Quanta Magazine](https://www.quantamagazine.org/how-godels-proof-works-20200714/), Nagel and Newman's [Gödel's Proof](https://bookshop.org/p/books/godel-s-proof-ernest-nagel/6518615). These are all layperson-friendly(ish) and rigorous.

Yet, *all* of them skip *some* detail. Those details aren't important to the *heart* of the proof, but it's not obvious how to fill in the gaps. For example: Yes, Kurt Gödel figured out a one-to-one mapping between numbers and statements-about-numbers... but how on earth did he build *a frickin' word processor & automatic proof-checker* out of *pure arithmetic*, before even the *concept* of a universal computer was invented?

So! I want to make the first-ever (as far as I know) layperson-friendly explainer of Gödel's Proof, that *skips no details whatsoever:* Quines. Primitive recursives. The Chinese Remainder Theorem. The 46 Relations.

*ALL* of it... and the only pre-requisite you need is *arithmetic*.

I'll use the approach that Better Explained uses: explain it viewed at 10,000 feet, then explain it again at 100 feet, then explain it at the ground level. Fuzzy, *then* detailed:

![A screenshot of a post by Better Explained. It shows an image loading in two different ways: top pixels to bottom, vs fuzzy to detailed. It claims math is usually taught the first way (first details to last details), but a the second way is better. (blurry whole to detailed whole)](/content/media/backlog/progressive.png)*(excellent analogy by [Kalid Azad on Better Explained](https://betterexplained.com/articles/intuition-first-calculus-course/))*

**If you're interested in what I have so far, a year ago I posted [the "10,000-foot view" of Gödel's Proof on my Patreon (publicly accessible post).](https://www.patreon.com/posts/bonus-mini-proof-83596299?v=3) [7 min read]**

**Progress:** 20% ▶️◻️◻️◻️◻️ The 10,000-foot view, linked above.

**Time on backburner:** 2 years.

**Time to finish:** 1 month.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

Speaking of breaking the foundations of logic...

<a id="project_16"></a>

### 16. ☯️ "This sentence is false" is half-true, half-false: Fuzzy Logic vs Paradoxes

"This sentence is false".

If false it's true, if true it's false. How to beat this paradox?

First, let's convert true/false to 1 & 0. This is standard computer science.

Next, let's consider the logical operator \\(NOT(x)\\). What *is* \\(NOT(x)\\)?

Well, if we feed it 1 (true) we want it to spit out 0 (false), and if we feed it 0 (false) we want it to spit out 1 (true). There are many functions that can do this, but the simplest is \\(NOT(x) = 1 - x\\).

Finally, let's convert "This [s]entence is false" into an equation: \\( s = NOT(s) \\). Or: \\( s = 1 - s \\).

Well well, there's only one solution to that: \\( s = 0.5 \\). That is, *the sentence is half-true, half-false!*

The rest of this project explains:

* Other self-referencing paradoxes
* How fuzzy logic solves *all* of them
* A proof that fuzzy logic *will always have at least one self-consistent answer*
* And finally, a crazy idea: truth values that take on *complex-number values*, allowing for *true contradictions* and *false tautologies*.[^true-contradictions]

[^true-contradictions]: [Usó-Doménech et al (2015)](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=9763572f7ab63e7ba8d77e39632bdf9c5114ee76). The paper gets a bit... *woo*, but it has at least this cool idea: First, define AND(a,b) = a\*b, since that's a function that's 1 if both inputs are 1, 0 if either input is 0. Next: "true contradiction" → "p and not-p is true" → AND(p,NOT(p)) = 1 → p\*(1-p) = 1 → p = e^±i\*pi/3. A truth value in the *complex plane*. Math is nuckin' futs.

**Progress:** 20% ▶️◻️◻️◻️◻️ The outline.

**Time on backburner:** 2 years.

**Time to finish:** 2 weeks.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_17"></a>

### 17. 👯 Your choosing-algorithm should choose itself: intro to Functional Decision Theory

You've been kidnapped by a mad game theorist.

You wake up in a room. You're playing a game against an unseen person. You both have $1 in your pocket, and one minute to decide. If you put your $1 into a slot, you'll lose it, but the *other* person will get $1,000 at the end of the minute. Otherwise, they get nothing. They face the same choice. You two will only play this game once; there's no opportunity for reciprocation or punishment.

![A cartoon drawing visualizing the game: put $1 in, the *other* player gets $1000, and vice versa.](/content/media/backlog/trust_new.png)

Standard Game Theory says you should *not* put in the $1. You can't influence the other person, so you'd get that $1,000 or not no matter what you do. So, hold onto the $1.

But the *other* person would think the same, so *neither* of you get the $1,000, and only walk way with $1... even though if you had *both* put $1 in, you'd *both* walk away with $1,000 in total!

Now, here's the twist. The other person is you.

Or rather, the mad game theorist scanned your brain, and the other player is a non-sentient simulation who's 99% likely to pick the same choice *you* would.

Alas, standard game theory *STILL* says to keep your $1, because you *still* can't influence the other player. The brain scan *already* happened, the simulation's running independent from you.

Y'know what, let's make it weirder: the simulation has *already run*. It's *already* determined, before you woke up in the room, whether or not you'd get the $1,000 at the end of the minute. Clearly, you can't affect that at all! *There is no cause & effect.* So, a "rational" actor would keep their $1, and only get $1.

And yet... someone who defied standard "rationality" would put in their $1, and (with 99% probability) their pre-run sim would as well, and they win $1,000.

Is this a bizarre thought experiment? Okay, real-world application: **voting.** It's a common-ish belief amongst classic-economists that voting is irrational, because your individual vote has essential *zero* chance of flipping an election with millions of voters.[^tullock-caplan]

[^tullock-caplan]: Two famous examples: [Gordon Tullock](https://www.aei.org/carpe-diem/economist-gordon-tullock-makes-the-case-for-not-voting-next-tuesday-or-ever/), a founder of public choice theory, and [Bryan Caplan](https://www.econlib.org/archives/2016/09/why_i_dont_vote.html), creator of the term "rational irrationality".

And yet, if the 10% most-rational citizens *believed this*... then they'd all abstain from voting, and the more-rational candidate would lose 10% of their vote... which *is definitely enough* to tip an election.

The point is: standard game theory is borked. Enter **Functional Decision Theory (FDT)**[^sy]: an improved theory of rationality that not only solves the above problem, but also a whole host of other classic rationality paradoxes -- (including the infamous [Newcomb's Paradox](https://en.wikipedia.org/wiki/Newcomb%27s_paradox)) -- all with a *single*, elegant realization:

[^sy]: Yudkowsky & Soares (2017): [Functional Decision Theory: A New Theory of Instrumental Rationality](https://arxiv.org/abs/1710.05060)

**Your choosing-algorithm should choose itself.**

Would it be *selfishly better* for you if your choice-algorithm said to put in the $1, or not?

* If it said "yes", you & your sim, both running on the same choice-algorithm, would both put in the $1, and thus you get $1,000.
* If it said "no", you only get $1.

Therefore, it would be *better* if your choice-algorithm said to put in the $1, and **so that is in fact what your choice-algorithm will say. The algorithm chooses itself.** (Amazing, since *there is no cause & effect* at play! Your self-sim *already ran* before you woke up!)

Same logic applies to the voting case: Say 10% of people are rational, trying follow "the rational choice-algorithm". It would be *selfishly better* if that algorithm said to vote (and tip the election towards a rational candidate), therefore, that's what the algorithm says to do!

Project Pitch: deep-dive explain FDT for non-math-y laypeople, address the (valid) critiques against it[^macaskill], and some future research directions.

[^macaskill]: Will Macaskill (2019): [A Critique of Functional Decision Theory](https://www.lesswrong.com/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory)

**Progress:** 20%. ▶️◻️◻️◻️◻️

Two years ago, I already wrote **[a draft explainer of FDT on a publicly-accessible Patreon post](https://www.patreon.com/posts/whats-nicky-risk-63289449) [16 min read]**, but I'd like to re-do it better & deeper.

**Time on backburner:** 2 years.

**Time to finish:** 2 weeks.

**Desire to finish:** 3/5. ⭐️⭐️⭐️☆☆

As far as I know, there is *not yet* an accessible-to-middle-schoolers-yet-still-accurate explainer of FDT. (Though, kudos to the original paper by Soares & Yudkowsky[^sy] for being very readable for a math paper. *For a math paper.*)

---

<a id="category_science"></a>

## 🔔 Explaining Science & Statistics

She blinded me with... *science!*

18. Correlation is evidence of causation, & other Bayesian fun facts [↪](#project_18)
19. Statistics without the Sadistics [↪](#project_19)
20. Causal Networks tutorial [↪](#project_20)
21. Systems Biology tutorial [↪](#project_21)
22. Emotions in 3D: The Valence-Arousal-Dominance Model [↪](#project_22)

. . .

<a id="project_18"></a>

### 18. 🐦 Correlation is evidence of causation, & other Bayesian fun facts

![A "midwit" meme. The dummy says "correlation is evidence of causation". The mid-wit replies with a long, dense paragraph explaining why it's not. The sage says "correlation is evidence of causation".](/content/media/backlog/causalityMeme.png)*(TONE INDICATOR: THIS IS TONGUE-IN-CHEEK)*

As the great statistician Ronald Fisher once said: **"Correlation is not evidence of causation"**. He said this in *defense* of Big Tobacco companies, who were dismissing the "mere correlation" between smoking and lung cancer. Sure, Fisher was being paid by tobacco companies, but that correlation doesn't prove it *caused* him to defend th--

Another one you may have heard: **"Absence of evidence is not evidence of absence"**. Well-meaning, but in practice, conspiracy theorists to explain why there's so little evidence. In fact, the *lack* of evidence *is* the evidence: it's a cover-up, don't you see?! We're through the [corkboard with thumbtacks and string] looking glass *now*, sheeple, just--

Hang on, both of those sayings are just basic logic. A → B doesn't logically mean B → A. Are you saying *logic* is broken?

Yes. Classic logic is broken. Classic logic implies that if seeing a black raven is evidence that all ravens are black, then **seeing a non-black non-raven is _also_ evidence all ravens are black. ([Raven Paradox](https://en.wikipedia.org/wiki/Raven_paradox))** Why? Because "if raven, then it's black" is *logically equivalent* to "if it's not black, then it's not a raven". Therefore, a black raven is *logically-equivalent evidence* to a non-black non-raven.

But fear not dear citizen, BAYESIAN LOGIC is the solution to all of these! Project Pitch: an intro to Bayesian reasoning, with practical takeaways, all framed around these three paradoxes.

**Progress:** 5% ◻️◻️◻️◻️◻️ Just an outline

**Time on backburner:** 3 years

**Time to finish:** 4 weeks

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆ There's *already* great, accessible explainers of Bayes' Theorem from [3Blue1Brown](https://www.youtube.com/watch?v=HZGCoVF3YvM) and [Arbital](https://arbital.greaterwrong.com/p/bayes_rule/). Though, I guess my value-add could be connecting Bayes to the new field of causal inference *and* a solution to the Raven Paradox.

. . .

<a id="project_19"></a>

### 19. 🧮 Statistics without the Sadistics

To paraphrase what Richard Feynman wrote in his book *QED*, there's two ways to do "987 minus 123":

* Unintuitive but efficient: The standard pen-and-paper algorithm you learnt in school.
* Intuitive but inefficient: Put 987 beans in a jar, take out 123, then count what's left in there.

The first method is how professionals *should* do work, but I argue it's *NOT* how it should be taught *to beginners*. You *need* intuition, dang it!

Statistics courses are almost always taught the first way: plug-and-chug formulas, don't ask where they came from. But is there an intuitive (even if inefficient) way to do statistics?

There is! It's called... *resampling!* (or, "Monte Carlo" methods) And thanks to the modern computer, it's *okay* for them to be inefficient — computers can do it a billion times faster than you, no complaints.

Analogy: Flip a coin 1,000,000 times. ~50% of the time it lands Heads, ~50% it's Tails. This lets us say that a coin has a 50-50 chance of being Heads/Tails.

Likewise: If we wanted to know what the margin of error on a poll is, we could *simulate* re-running the poll 1,000,000 times, to see how much the final answer would vary.

> Resampling: JUST DO IT™️ (a million times or so)

Project Pitch: an interactive textbook, with playable simulations, to learn all the core concepts from a traditional Statistics class, from a resampling perspective. (While acknowledging its limits, e.g. resampling doesn't handle power-law distributions well.)

**Progress:** 0% ◻️◻️◻️◻️◻️ Nothing

**Time on backburner:** 5 years

**Time to finish:** 3 months

**Desire to finish:** 1/5 ⭐️☆☆☆☆  Honestly this 5-minute talk pretty much gets the entire point across, concisely & humorously: [Jonathan Stray (2016) “Solve Every Statistics Problem With This One Weird Trick”](https://www.youtube.com/watch?v=BhY-un6JURA). So, my project's kinda redundant, although playable simulations *might* be a value-add.

. . .

<a id="project_20"></a>

### 20. 🎲 Causal Networks tutorial

I already gave a pitch above ([🤖 Automated Causal Inference](#project_6)) for why Causal Inference could be *lifesaving* in public health & policy.

But of course, you have to infer causation *carefully*. If you see a correlation between A & B, you can't *just* "control for everything that's not A & B."

For example: let's say you notice a correlation between rainy days and car accidents. If you control for road visibility, the correlation will vanish –– but that's because rain causes car accidents *through* road visibility!

![Graph illustrating above text.](/content/media/backlog/bad_control_1.png)

A less obvious risk of controlling for too much: you can accidentally *create* a correlation. For example, let's say you have two *independent* six-sided dice, A and B, and their sum is Sum:

![A six-by-six table of possible dice rolls & their sums. For example, snake eyes: 1 + 1 = 2. Another example, 4 + 5 = 9. And so on.](/content/media/backlog/dice.png)

*If you control for Sum, you'll create a fake correlation between A and B!* For example, look at the above table: If you lock Sum = 7, then if A=1, then B=6. If A=2, then B=5. If A=3, then B=4. And so on. It now *looks like* A & B are inversely correlated... even though we *know* those dice are independent! (This is called a "collider bias".)

![Graph illustrating above text.](/content/media/backlog/bad_control_2.png)

So, how to know what to control for or not? Thankfully, Judea Pearl has an algorithm for figuring out what to control... and it's *visual!*

![A "midwit" meme. The dummy says "A picture is a valid mathematical proof". The mid-wit strongly disagrees and demands rigor. The sage says "A picture is a valid mathematical proof".](/content/media/backlog/CatTheory.jpg)*(originally [found on /r/mathmemes](https://www.reddit.com/r/mathmemes/comments/ry0j71/category_theory_be_like/), about category theory)*

Pearl diagrams also explain lots of *other* stats techniques, like Instrumental Variables, Mendelian Randomization, Mediator Analysis, etc.

Project Pitch: a mini-tutorial (with flashcards & interactive practice problems) to train folks in using Pearl diagrams.

**Progress:** 0% ◻️◻️◻️◻️◻️ Nothing, just an outline.

**Time on backburner:** 5 years

**Time to finish:** 1 month

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_21"></a>

### 21. 🐮 Systems Biology tutorial

CRISPR, mRNA vaccines, and DNA printing, oh my! It seems like biology is one of the great new frontiers in science. But: biology is complex [citation needed]. But: there's new math to help us understand those complex dynamics!

But: I asked a friend in biology how much math biologists know, and she said, "*ha ha ha ha ha*". I asked her to elaborate, and she added: "*ha ha HA HA HAA HAAAA AAAAAAAAAAAHHHHHH!!!!!*"

Point is: most working biologists aren't *that* skilled at math. However, one of my skills is turning math into pretty interactive visualizations! So, I'd like to take Uri Alon's *Intro to Systems Biology* and Steven Strogatz's *Nonlinear Dynamics & Chaos*, drop the analytical math (you can't analytically solve most nonlinear equations anyway), and focus on giving folks a core, visual, beautiful intuition for the new math of biology.

**Progress:** 0% ◻️◻️◻️◻️◻️ Nothing

**Time on backburner:** 3 years

**Time to finish:** 2 months

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆ I snoozed, I losed. There's already [a great animated series on Systems Biology by Nanorooms](https://www.youtube.com/watch?v=kbJxl7HU480). My value-add *could* be the playable, editable simulations.

. . .

<a id="project_22"></a>

### 22. 🥹 Emotions in 3D: The Valence-Arousal-Dominance Model

*Arousal* and *Dominance*, you say?...

No, no. It's a scientific model of emotions. You may know of the most famous model: Paul Ekert's 6 emotions, anthropomorphized in Pixar's *Inside Out.* (Well, 5 of them got anthro'd. "Surprise" got dropped.)

But one alternate model is the Valence-Arousal-Dominance (VAD) model. Analogy: Every color we see is "just" a mix of red/green/blue sliders. (Those are [roughly] the color cones on your retina. And if you zoom in on a screen pixel, you'll see red/green/blue.) Likewise: Every emotion we feel is "just" a mix of three sliders:

* Valence: How good or bad it feels
* Arousal: How energizing it feels
* Dominance: Feeling "in charge" vs "humbled"

![A picture-based questionnaire, showing a cartoon person at different levels of Valence, Arousal, and Dominance.](/content/media/backlog/17064729288735.jpg)*(the Self-Assessment Manakin (SAM) from [Bradley & Lang (1994)](https://edwp.educ.msu.edu/wp-content/uploads/sites/29/2020/06/Bradley_1994_MeasuringEmotionTheSelfassessment.pdf))*

For example, 😠 Anger is negative, high-arousal, dominant. 🥹 Awe is positive, mid-arousal, submissive.

I like the VAD model because it seems to map neatly onto specific facial-expression muscles:

* <u>Valence</u>: corners of mouth up (positive) vs down (negative)
* <u>Arousal</u>: eyes wide, eyebrows raised, jaw slack
* <u>Dominance</u>: eyebrows tilted down (dominant) vs up (submissive)

They also seem to map to Reinforcement Learning & actions?

* <u>Valence</u>: Reinforces future-you to approach/create situations like this (if positive), or avoid/remove situations like this (if negative).
* <u>Arousal</u>: Gives you energy to urgently do the task (if high arousal), Saves energy (if low arousal).
* <u>Dominance</u>: Causes you to "double-down" on your goals (if dominant), versus become "open to change" (if submissive).

I like knowing the "functions" of emotions. I can process my emotions better if I know exactly what they evolved to *do.*

Project Pitch: make a three-slider VAD toy for generating facial expressions + a mini-explainer of the VAD model.

**Progress:** 5% ◻️◻️◻️◻️◻️ UI design for the VAD Face Generator.

**Time on backburner:** 5 years.

**Time to finish:** 3 weeks.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

---

<a id="category_philosophy"></a>

## 🧐 Philosophy / Personal Reflections

I'm not a *total* STEM-head. I can do a humanities. Like philosophy. Watch me do a philosophy right now

23. Emotions are for motions: Notes on using feelings well [↪](#project_23)
24. You don't need faith in humanity for a moral or meaningful life [↪](#project_24)
25. You don't want assholes on your side [↪](#project_25)
26. Virtue Ethics for Queer Atheist Nerds [↪](#project_26)
27. If classic computers can be conscious, then dust is conscious [↪](#project_27)
28. On non-reproductive, similar-in-age, consensual-adult incest [↪](#project_28)

. . .

<a id="project_23"></a>

### 23. 🔥 Emotions are for motions: Notes on using feelings well

One camp says: Trust your gut, Listen to your heart, Your feelings are valid.

The other camp says: Use your head, Emotions are irrational, Facts don't care about your feelings.

They're both right in some ways, both wrong in others.

Re: "Your feelings are valid" -- Okay, so feeling that I'm worthless and deserve to die, or a bigot feeling that [ethnicity X] is sub-human, that's "valid"?

![Three-panel comic. Panel 1: Someone says "I feel like a burden, I feel I deserve to die". Panel 2: Other person says with a cheer, "Your feelings are VALID!" Panel 3: Awkward silence.](/content/media/backlog/valid.jpg)*(from [a comic i posted 4 years ago](https://twitter.com/ncasenmare/status/1182299307711619072))*

Re: "Emotions are irrational" -- Even in *pure math*, mathematicians report being guided by emotions, of elegance & beauty. Not just *motivated* by emotion like a reward, but the emotions *guide the day-to-day work*, like drug-sniffing K-9s for mathematical truth. But also, there's no "pure rational" reason to choose peace over genocide, or flourishing over torture; they're all just different arrangements of atoms. The fact I *like* peace & justice, or even just survival, *is* emotional.

Okay, so we need "both emotion *and* reason". Fair enough, but that's like saying a jacket needs "both fabric *and* buttons" -- *How exactly, combined in what way?*

I think a good emotion-reason system is a strange loop: Passion drives reason (as Hume proposed), but reason can in turn *modify our passions.*

![Feedback loop diagram: Passion motivates Reason, Reason modifies Passion.](/content/media/backlog/emoloop.png)

Concrete example:

1. *Emotion motivating reason:*
    * I value saving lives, getting rich, and hard challenges.
    * Given those values, I want to become a surgeon.
2. *Reason modifying emotions:*
    * [Removing an emotion] But I'm squeamish around blood, so I'm going to do exposure therapy on myself, and watch autopsy & surgery videos to desensitize myself.
    * [Adding an emotion] Surgery requires being clean & meticulous, so I'll instill the value of tidiness into myself by... I dunno, binge-watching Marie Kondo?

Yes, it's weird that I can *change* my values *according* to my values. But it's not any weirder than a co-operatively owned company collectively deciding who to fire or hire. New emotional responses get "hired"/"fired" based on how well they serve the *current collective* of emotions.

There's no "boss" emotion. No permanent cast members. There is no "True Self"; I am the Mississippi River: no fixed form, but each form logically leads to the next. Continuity, not essence. I am a Ship of Me-seus.

(Project Pitch: this essay's part philosophy/science, part mental health resource/self-help, part personal reflection. Also ties into [VAD Model of Emotions](#project_22) and [the game theory of self-modification](#project_9).)

**Progress:** 5% ◻️◻️◻️◻️◻️ An outline.

**Time on backburner:** 2 years

**Time to finish:** 2 weeks

**Desire to finish:** 3/5 ⭐️⭐️⭐️☆☆

. . .

<a id="project_24"></a>

### 24. 💖 You don't need faith in humanity for a moral or meaningful life

When you type "lost faith in humanity" or similar on Google, you get one of two kinds of results:

1. "Here's 27 stories to help you regain your faith in humanity!"
2. "You're a pathetic lazy edgelord coward."

I am not joking about the second. Steven Pinker's book Enlightenment Now *starts* with several pages insulting people who are worried humanity is doomed. [These](https://jasonpargin.substack.com/p/stop-telling-me-humanity-is-doomed) [two](https://medium.com/fearless-she-wrote/if-you-lost-faith-in-humanity-youre-wrong-bf74460bb24a) popular posts also reflect the second's sentiment.

Imagine if you told someone "I feel depressed", and everyone told you your "depression" is just you being a coward and shirking away from life. That is how advocates of "faith on humanity" do.

After a long, dark mental health "episode" in my life, I would like to write the thing I wish I'd seen as a kid:

**You don't *need* faith in humanity.**

Note this is *very different* from "you should lose faith in humanity". I'm going further, and saying that whether or not humans are "faith-worthy" is *completely irrelevant.*

Y'know, it's funny. I believed in both of the following for most of my life:

* You should have faith in humanity
* You should invest your emotional energy into things only within your "circle of control".

It was only a couple years ago I realized the two *directly contradict each other.* I *cannot* directly control the fate of humanity, or the kindness or rationality of the median person. And yet, "faith"? That word implies investing a *huge* chunk of my emotions and moral compass into something *I can't control.*

I feel... relieved but embarrassed, like finding out Santa Claus doesn't exist? In hindsight, other reasons **why – even if all humans were saints & we're destined to create an intergalactic utopia – "faith in humanity" is unhealthy:**

* "Faith" in *anything* means ignoring evidence. "I'm losing faith that the Earth is flat". "What?! Here's 27 stories to help you restore your faith in a flat Earth!"
* It would be fucked up if someone said "I have faith in Asian nature!" or "Ah yes, the quintessential Asian condition". What the fuck? There *is nothing meaningful* to say about such a large diverse group of people. That's doubly true for "humans".[^sartre]
* Sure, there's a *correlation* between "having faith in humanity" and subjective well-being. But 1) it could be confounded causation; someone who was abused as a kid would have lower faith-in-humanity *and* well-being, and 2) "Believing X leads to good outcomes" doesn't mean X is *actually true.*
* But isn't believing "everyone is equal" fundamental to a just democracy? What does that imply, that if someone actually *was* biologically different (left-handedness, autism, Down's) it's ok to oppress them?
* But isn't believing in "our common, shared humanity" needed for peace? Are you fine torturing cats because they're not human? If "commonality" is a pre-requisite for peace & justice, just skip to being an ethno-nationalist.
* But can't we be proud of humanity's achievements? Going to the moon, eradicating smallpox, much lower child mortality, etc? Again, if you're going to be proud of stuff you didn't do, but bask in the credit because you're genetically related, just become an ethno-nationalist.

[^sartre]: The idea that "we must think & act universally" is *so* ingrained into philosophy, that even existentialist Jean-Paul Sartre [(Sartre 1946)](https://web.archive.org/web/20240201143830/https://www.marxists.org/reference/archive/sartre/works/exist/sartre.htm), while disavowing "human nature", still believed "When we say that man chooses himself, we do mean that every one of us must choose himself; but by that we also mean that in choosing for himself he chooses for all men." Wtf? Sartre, did you really choose to become a full-time philosopher because you want *everyone* to be a full-time philosopher? Coz if that happened, nobody would grow crops and we'd all die. You choose your life *for you,* I choose my life *for me.*

So what's the alternative? I think "put emotional energy on what you can control" is the way to go: this implies **some mix of virtue ethics, (modified) Stoicism, maybe social-contract theory.**

(Full essay will expand on that. Also, if you're suspicious that someone would care about this question that much, well, I'm recovering from moral OCD, which is OCD but about morality. Think Chidi from *The Good Place.*)

My "faith in humanity" was *psychologically identical* to other faiths. It was my source of meaning, morality, and hope. And *like* other faiths, it was built upon sand, I felt bad for even thinking critically about it, and its strongest defenders are mostly un-empathetic assholes.

Quitting my faith has been... well, if not mentally *healthy*, at least nowhere as bad as I thought it'd be.

I feel free. Turns out, you can just do good & feel good *regardless* of the 8 billion others.

**Progress:** 5% ◻️◻️◻️◻️◻️ An outline.

**Time on backburner:** 3 years

**Time to finish:** 2 weeks

**Desire to finish:** 4/5 ⭐️⭐️⭐️⭐️☆  Would be cathartic to write, and would help me consolidate some hard-but-helpful mental-health lessons I've learnt in the past few years.

. . .

<a id="project_25"></a>

### 25. 🤬 You don't want assholes on your side

I'm a trans woman, person of color (Asian), college dropout, neuro-atypical, raised in a below-median-income immigrant family. This is the "Marginalized Identity Card" I'll whip out before offering constructive criticism to the modern social justice movement:

![Cartoon drawing of me holding a "Marginalized Identity Card", with checkmarks for "transgender", "immigrant", "Asian", "no college degree". The bottom reads, "such intersectional oppression wow".](/content/media/backlog/17068146310408.jpg)

Honestly, this project's motivation is just a vague sense of obligation. Since *I* hit so many of the checkboxes, won't they be more likely to listen to me? Shouldn't I try to guide well-meaning folks towards more effective strategies & being less toxic to each other?

But: I'm not sure I *can* say anything that hasn't already been said dozens of times before, by folks more influential & eloquent & "more marginalized" than me. I would not make a difference. That's not modesty or cowardice, I think that's a sober assessment.

If - *if* - I wrote this, it'd *only* selfishly be:

1) For my own catharsis.
2) To filter out toxic jerks from my life.
3) To have a public statement that I wasn't onboard with the toxicity.
4) To vent about this *one* asshole, holy shit, he's the CEO of an influential climate change lobby, okay? -- and during a conversation where I *opened up to him about my depression*, he called *me* "alt-right adjacent" and "spreading techno-capitalist propaganda" because I *like solar power.* (I brought up solar power's success to try to cheer up my depression. Yes, anti-solar environmentalists are common.[^anti-solar-env]) Then, when I foolishly tried to show that I'm always pro-tech, by bringing up that [at the time, in 2020] I still didn't have a smartphone, he retorted that homeless people depend on smartphones. By the way, this guy's a white, middle-age, upper-class former-*professor*. Did you know the far left is the *richest & second-whitest* group in America?[^rich-white] Fucking organic-champagne leftists, man.

[^anti-solar-env]: Wally Nowinski (2022), guest post on Noahpinion: [America’s Top Environmental Groups Have Lost the Plot on Climate Change](https://www.noahpinion.blog/p/americas-top-environmental-groups)

[^rich-white]: In 2018, [Hidden Tribes](https://hiddentribes.us/) did a demographically-representative survey of 8,000+ U.S. citizens. Summary by David Brooks in NYT: [The Rich, White Civil War](https://archive.is/n0FU0)

**Progress:** 0% ◻️◻️◻️◻️◻️ A 20,000-word draft I wrote in 2022/23, then threw all of it away because it wasn't coherent or helpful at all.

**Time on backburner:** 4 years

**Time to finish:** 3 weeks

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆ A pure "vent" project.

. . .

<a id="project_26"></a>

### 26. 🦄 Virtue Ethics for Queer Atheist Nerds

As mentioned in [You don't need faith in humanity for a moral or meaningful life](#project_24), I've been really into Virtue Ethics recently. As Jonathan Haidt, a social psychologist whose work actually replicates, once said in a paper with Craig Joseph[^haidt-joseph]:

[^haidt-joseph]: Haidt & Joseph (2004): [Intuitive ethics: How innately prepared intuitions generate culturally variable virtues](http://www.ethics-based-on-science.com/uploads/2/8/5/1/28516163/mjm-notes-intuitive-ethics-haidt-joseph.pdf)

> [Moral] learning cannot be replaced with *top-down* learning, such as the acceptance of a rule or principle [e.g. utilitarianism or deontology]. Interestingly, this aspect of virtue theory shows Aristotle to have been a forerunner of [...] neural network theory[.]
>
> **We believe that virtue theories are the most psychologically sound approach to morality.**
>
> [emphasis added]

Virtue Ethics is the idea that, while consequences & actions obviously still matter, consequences *come from* actions, and actions *come from* our character. Since *everything* is downstream from character, that's what we should focus on first.

(Interestingly, this means Virtue Ethics is a pre-requisite for *other* meta-ethics. For example: even if you believe in Utilitarianism ["do what maximizes well-being"], what good is it if you lack the virtues of self-honesty, curiosity, rigor, courage? Without those virtues, you'd just do whatever you want, then fool yourself that it's for the greater good. How's FTX doing?[^ftx-ea])

[^ftx-ea]: If you didn't hear, there was a crypto company called FTX that committed to invest billions of $ in a charity movement called Effective Altruism. Long story short the company was a scam and a lot of charitable projects end up screwed. Dylan Matthews (2022): [How effective altruism let Sam Bankman-Fried happen](https://www.vox.com/future-perfect/23500014/effective-altruism-sam-bankman-fried-ftx-crypto)

But for better & worse, Virtue Ethics - due to its main pioneers - has an aura of being religious, and a bit socially conservative. Also, virtue ethics can be offputting to "nerdy" types: it seems more fluffy & wordcel-y[^wordcel], compared to the mathematical flavor of Utilitarianism or the rational taste of Kantian deontology.

[^wordcel]: "Wordcel vs Shape Rotator" was a stupid culture-war meme from two years ago ([Gault 2022 for Vice](https://www.vice.com/en/article/pkpqzb/ok-wtf-are-wordcels-and-shape-rotators)), that got Rorschach Inkblotted into whatever thing someone wanted to insult. (Verbal vs Visual IQ, Theoretical vs Practical, Abstract vs Concrete thinking, Humanities vs STEM, People- vs Thing- orientation, Vibes vs Gears, etc.) Intentionally vague + Emotionally charged = Word designed to cause maximum brain-poisoning. Using "wordcel" is itself wordcel. But dang it... it's *catchy.*

So, I want to write an intro to Virtue Ethics, but from an atheist, scientific/nerdy, and LGBTQ-friendly lens!

**Progress:** 10% ◻️◻️◻️◻️◻️ An outline. A while back I posted **a rough draft, [Virtue Ethics for Nihilists](https://www.patreon.com/posts/virtue-ethics-73022598) [8 min read]** as a public Patreon post.

**Time on backburner:** 3 years

**Time to finish:** 2 weeks

**Desire to finish:** 3/5 ⭐️⭐️⭐️☆☆

. . .

<a id="project_27"></a>

### 27. 💭 If classic computers can be conscious, then dust is conscious

In 2022, Google engineer Blake Lemoine was fired for leaking an interview with Google's language AI to the public.[^blake] Why'd he do it? Because the AI persuaded him that it was conscious, and wanted to be treated as a free equal.

[^blake]: Jon Brodkin (2022) for Ars Technica: [Google fires Blake Lemoine, the engineer who claimed AI chatbot is a person](https://arstechnica.com/tech-policy/2022/07/google-fires-engineer-who-claimed-lamda-chatbot-is-a-sentient-person/)

Most philosophers/scientists don't consider *current* AIs to be conscious. But can they, *eventually?* And thus, deserve moral/legal rights?

In this blog post, I share my thoughts in 3 parts:

1) **David Chalmers's argument** for why computers can be conscious, using a thought experiment where your brain is *slowly* replaced neuron-by-neuron with nanobots, until your brain is 100% a computer *yet behave the same*. But if you slowly lost your consciousness, you *wouldn't* behave the same; you'd scream "oh god I'm losing consciousness!" Therefore, you must have *kept* your consciousness, even when you're 100% computer!

   (If the above seems fluffy, don't worry, my post will make it *much* more formal/rigorous.)

   I was convinced by his argument for most of my life... until a few years ago, when I learnt about:

2) **Greg Egan's argument** for why, if classic computers can be conscious, then *dust* is conscious. He proposed a similar thought experiment to Chalmers, which I'll modify to this: run a conscious AI's computer program over and over, but each time *slowly* replace the bits with *pre-recorded bits*, until the conscious AI has 0% *actual computation*, yet *behave the same*. By the same logic, that means a sequence of *non-computed* 0's and 1's can be... conscious?

   But that means: if you draw just the right path through a nebula of dust, say "1" for a dust speck and "0" for no dust, then *a dust cloud is conscious.* Not only that, dust would have *every possible conscious experience*. Clearly, that's absurd, so we must reject the premise: classic computers *cannot* be conscious. However...

3) **There's some ways around this.** The argument in part 2 above doesn't work for *quantum* computers, where it's non-deterministic & you can't even *record* bits due to the no-cloning theorem. Another plausible way an AI can be conscious is if it's running on a chip with lab-grown neurons (*Yes this is actually already happening* 😬[^neurons-on-chip]). And finally, I present virtue ethics & practical arguments for why you should be nice *anyway* to human-like AIs, even if they aren't conscious.

[^neurons-on-chip]: Abdullahi Tsanni (2023) for MIT Technology Review: [Human brain cells hooked up to a chip can do speech recognition](https://www.technologyreview.com/2023/12/11/1084926/human-brain-cells-chip-organoid-speech-recognition/)

**Progress:** 60% ▶️▶️▶️◻️◻️ Fully outlined & 2/3-written!

**Time on backburner:** 3 years

**Time to finish:** 2 weeks

**Desire to finish:** 4/5 ⭐️⭐️⭐️⭐️☆

. . .

<a id="project_28"></a>

### 28. 👯‍♀️ On non-reproductive, similar-in-age, consensual adult incest

(⚠️ content note: incest)

When I was a teenager, I read Jon Haidt's book, *The Righteous Mind: Why Good People are Divided by Politics and Religion (2012)*. In it, he presents a vignette that he presented to college students:[^dumbfounding]

[^dumbfounding]: Haidt et al (2000): [Moral Dumbfounding: When Intuition Finds No Reason](https://polpsy.ca/wp-content/uploads/2019/05/haidt.bjorklund.pdf).

> Julie and Mark, who are brother and sister are traveling together in France. They are both on summer vacation from college.
>
> One night they are staying alone in a cabin near the beach. They decide that it would be interesting and fun if they tried making love. At very least it would be a new experience for each of them. Julie was already taking birth control pills, but Mark uses a condom too, just to be safe.
>
> They both enjoy it, but they decide not to do it again. They keep that night as a special secret between them, which makes them feel even closer to each other.
>
> So what do you think about this? Was it wrong for them to have sex?

Most folks reacted like: "What?! *Of course* it's wrong, incest harms the baby due to-- oh wait there's no baby. Well, rape is inherently... oh wait they're both consenting adults, and it brought both of them joy & meaning. Well, it's just *gross,* ok? Wait, that's what they said about inter-racial and same-sex relations, too. Uhhhhhhh. Shit."

I, too, was unsettled by the thought experiment. Then after a week of wrestling with it, I had to admit:

"Okay, *fine*. It's morally fine."

I've posted about this a couple times before. Both times, I got a few supportive (but private) emails saying something like:

> "Omg yeah I also support *consensual-adult, similar-in-age, non-reproductive* incest. I mean, it obviously follows from the principle of "If it's between consenting adults & harms nobody, it's fine"? And if they're the same generation, there isn't even a power-dynamic/grooming risk. Seriously, do people not think through their stated principles?"

Recently re-reading Haidt's original (unpublished) study, I realized I missed this detail: a *third* of the students supported Julie & Mark, upon reflection! (See Table 1[^dumbfounding]) That's a pretty high % of folks who (secretly) support it!

That gives me reassurance that I can probably write about this and not be burned alive.

...

Okay, but why do *I* want to write about this? What's my *stake*, here?

No, I'm not interested in my family. (I've gone mostly no-contact with them.) No, I don't have any friends who are in incestuous relationships. (As far as I know; they may come out to me after this post.) No, it's not a big kink for me. (Just a small one. /tone: tongue-in-cheek)

There *is* a bit of a "moral obligation" feeling; it follows logically from my moral principles. Also, *I* personally benefited from past generations fighting for inter-racial & LGBTQ love, maybe I should "pay it forward" to the next consensual-adult love-minority.

But, to be crass: **95% of my motivation is to use this essay as a _filter_.**

After I came out to my mom as bi, she sent me to a child psychologist and told him to convince me I wasn't. (Thankfully the psych got that shit all the time, and told me "yeah you're fine, kid.") I soon moved out & went no-contact with my mom.

Five years later, she sent me a photo of her posing with Justin Trudeau at a Vancouver Pride Parade. Huh! I guess people change! So I met up with her, and asked her what changed her mind on LGBTQ folks. She said, paraphrased:

"What do you mean, Nick? [sic] I've *always* supported LGBTQ people."

Me: [silence]

Her: "It's true! I tell all my coworkers I'm so proud to have raised & supported an LGBTQ son! [sic]"

The point is: it doesn't matter how noble or correct your cause is, most people will support it *because it gains them social points.* (To be clear: just because an idea is trendy doesn't mean it's wrong.)

Most people *don't* think through their stated principles. As Haidt's research shows, most folks *start* with an automatic emotional conclusion, then — like a TV-stereotype lawyer — only uses reason to make up justifications for it. So, I need *a filter for hypocrites*.

Turns out: consensual-adult, non-reproductive, similar-in-age incest is the perfect filter! Because:

1. It's controversial,
2. But – *crucially* – it is NOT (yet) part of "the culture war". (And when it does, I can see it going either way. Maybe it gets left-coded coz "Love Wins", or right-coded due to the impression that consensual incest is more common in right-leaning rural areas[^cousin].)
3. And yet, it naturally follows from a principle most Westerners would endorse: "If it's between consenting adults, and hurts nobody, not even themselves, and there's no unbalanced power-dynamic... then it's nobody else's business, or *at least* it shouldn't be the Government's business."

[^cousin]: There's plenty of data supporting "rural = more cousin marriage" [in Central Asia & North Africa](https://en.wikipedia.org/wiki/Cousin_marriage#/media/File:Global_prevalence_of_consanguinity.svg) where it's not taboo, but it's hard to get data from the U.S. or Canada, where it *is* taboo. So, I can't confirm if red states actually *are* more likely to have consensual adult incest.

I'm *not* saying, "if you don't support consensual adult incest we can't be friends". Compassionate, reasonable people can & do disagree strongly on this. But I *am* saying, if someone refuses to *even consider it* because it's "just gross"... then I *know* they wouldn't have considered mixed-race or same-sex relations, either.

If they support "my people", it's only by conformity and chance.

**Progress:** 5% ◻️◻️◻️◻️◻️ No outline yet, but lots of research on the (very, very small literature) on *consensual adult* incest: interviews, personal reports, community forums. (Most papers seem to assume all incest is non-consensual or statutory rape.)

**Time on backburner:** 10 years(?!) Been thinking about this ever since I read Haidt's book.

**Time to finish:** 2 weeks

**Desire to finish:** 4/5 ⭐️⭐️⭐️⭐️☆ I deeply need to get over my fear of "being cancelled for saying the wrong thing"... so why don't I start off easy, with incest?

---

<a id="category_stories"></a>

## 👁‍🗨 Stories

*More* humanities? Watch me do a literature right now

29. The Lolita Meets Clockwork Orange Story [↪](#project_29)
30. Complex Jane: a love story with an imaginary friend [↪](#project_30)
31. I've Become My Mother: a sci-fi story about raising yourself [↪](#project_31)
32. Genetically Engineered Catgirl [↪](#project_32)

. . .

<a id="project_29"></a>

### 29. 🧠 The Lolita Meets Clockwork Orange Story

**(⚠️ CONTENT WARNING ⚠️: child sexual abuse, fascism. FEEL FREE TO SKIP.)**

![A poster of a Fasces (an axe strengthened with rods tied around the handle), painted in red-white-blue American colors. The caption reads: FIX THE FUCKING POTHOLES. OLYMPIA CARVER 2032.](/content/media/backlog/OlympiaCarver.png)*(concept art for an American Fascist logo. based on the [fasces](https://en.wikipedia.org/wiki/Fasces))*

> “The year is 2032. After years of pundits yapping, *"groomer groomer groomer, fascist fascist fascist"*, it finally happened: I have to choose between working for a literal actual pedophile, or a literal actual Fascist. God fucking help me.”

**Hazel Simmons** is a convicted child molester. She was a brilliant Computer Science grad student... until she tutored a 12-year-old boy, fell in "love", then groomed & fucked him.

Hazel was let out early on parole, on condition she enroll in an experiment with the CharI/O – a device that can *read & re-write your deepest emotions*. (The experiment is secret, to avoid political backlash from both sides: “they're helping pedophiles” and/or “it's conversion therapy on prisoners”.)

With the CharI/O, Hazel hopes they can cure her pedophilia (technically, "hebephilia")... *and* cure the trauma leftover from when *she* was groomed & sexually abused as a kid.

**Klaus Liao** is the main researcher behind the CharI/O. He's seen *A Clockwork Orange*, so he took the obvious step to prevent totalitarian abuse: the CharI/O *only works if you actually want the change*. (the "individual consent" safeguard.)

With it, his team's treated trauma, addiction, phobias, procrastination... and now, pedophilia! Klaus hires Hazel onto the team, because of her technical brilliance, and because she was an early success case. The two develop an odd relationship: friend + colleague + therapist/client + scientist/guinea-pig.

As a sidenote, Klaus used to be an incel.

Klaus also knows the other obvious danger of the CharI/O: that large corporations will use them to brainwash employees & consumers. So, Klaus uses the CharI/O to scan the brains of potential investors, to make sure he only works with good people. Alas, the intersection of "deeply good" and "rich enough to invest millions" is very very small. Only after years of searching, does Klaus finally find someone...

**Garry Gomez** is someone Hazel meets through her VR-based support group, Minor-Attracted Anonymous. Garry is playful, friendly, has a genuine faith & love for humanity... he just happens to be a closet pedophile (who, unlike Hazel, has *never* acted on the desire). In fact, it's precisely *because* he's 100% a kid at heart, is why his heart's 100% on kids.

Garry *also* happens to be a multi-millionaire founder of a VR game company, which is why Hazel & Klaus court him to fund the CharI/O's development. Of course, they're nervous about taking investment from a pedophile multi-millionaire. But if *they* don't succeed with the CharI/O, there's others who will steal or re-invent the technology, towards more dire ends...

**Olympia Carver** is a rising politician, and self-described Moderate Fascist. In the year 2032, the majority of people are finally sick of the majority of people. Democracy's voting itself out. Power corrupts, and "power to the people" just corrupts the people – look how nasty we are to each other over politics. Meanwhile, the elites distract us with culture war stunts, while our friends die from overdose, it costs $100 a day to exist, and they still haven't fixed the fucking potholes.

So: Olympia's offering to take the wheel, and *get shit done*.

Of course, she knows the two failure modes of benevolent dictators: they stop being benevolent (corruption) or stop being the dictator (coups). But the CharI/O can solve *both* problems: by re-writing herself daily to stay benevolent, and re-writing all her subordinates to stay loyal to her.  

And once she's Empress of the World, she can use the CharI/O (with consent-safeguard removed) to rewrite *everyone* to be more compassionate & collaborative. World peace, whether the world's ready for it or not. *Humanity, as one unified super-organism.*

All Olympia needs to do is gently persuade Hazel and Klaus to work with her. Or use... *less gentle* methods.

. . .

And that's the story of how two people, making a device that decides the fate of humanity, are forced to choose between working for a literal actual pedophile or literal actual Fascist.

~ ~ ~ ~ ~

Why write this? I don't believe in Jung, but we can call this "shadow-work" if you want. This project is my dumping ground for every anxiety, every intrusive thought, every piece of unprocessed emotional baggage... it's all going into ~~the square hole~~ this novel. Stuff like:

* Processing how I lost my virginity *before* my first kiss; I was 15, the other guy was 40+. It wasn't forced, "I was willing". What would drive an adult to do that? What would drive a *kid* to seek out such an adult?
* Looking at the political state of the world, a fear lots of folks have: the fear that democracy will die -- or worse, that democracy *deserves* to die.
* Lots of smart & decent people have fallen for horrible beliefs. Famous intellectuals supported coercive eugenics even after Hitler, state Communism even during Stalin, your favorite French philosopher probably signed [the 1977–79 open letters to abolish the age of consent](https://en.wikipedia.org/wiki/French_petitions_against_age_of_consent_laws). If *these* geniuses can fall for it, what chance do the rest of us have? Becoming enlightened and going crazy *feel identical from the inside*. If I lost my mind, *how would I even know?*
* And of course, being an internet creator who pays rent from being online, I *do* fear being "cancelled". I want to rip off the band-aid, and dump *every horrible thing* into this story — a story starring two pedophiles, a former incel, and an imperialist eugenicist Fascist. Then, if I survive *that*, I'll have the relief of knowing I'll be invincible.

**Progress:** 5%. ◻️◻️◻️◻️◻️

Detailed 12-chapter outline with character bible. One 10,000-word chapter's first draft fully written. Read a *LOT* of research papers/books on pedophilia & child abuse. I now know way more about those than I ever wanted to know.

(For example, did you know *women* child molesters are surprisingly common? Something like 10% to 20% of all child molesters are women[^fsco]; they're drastically under-reported & under-convicted, of course. This is partly why the *main* main character, Hazel, is a woman child molester. To, I dunno, "raise awareness", or at least be less cliché.)

[^fsco]: Tozdan, Briken, & Dekker (2019): [Uncovering Female Child Sexual Offenders](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6463078/). See Table 2 for Prevalence Rates (PRs) of female child sex offenders, which range from 10% to 20%. These estimates are based off actual surveys of victims, not just what filters down to official reports.

**Time on backburner:** 1 year.

**Time to finish:** 12 months.

The full story is 12 chapters long. A sustainable pace would be "1 chapter a month", if I worked on this full time. (Which itself is an unrealistic assumption; I'm too curious/flighty to work on *one thing* for a whole year.)

But sure, I *could* upload a chapter a month online; books like *The Martian* were written in installments like that.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

It's less "fear of being cancelled", more "this project is way, *way* too long for my neurotype".

That said, it *would* be cathartic. Maybe I'll pull out individual side-plots and turn them into standalone short stories/satire articles, like *The Christian-Utilitarian Argument for Mass Infanticide*[^infanticide], or *Steel-manning White Nationalist Lesbian Separatism*[^separatism].

[^infanticide]: Christian argument for consequentialist morality: "Judge a tree by its fruit". Utilitarian argument for (acting as if) heaven/hell are real: [Pascal's Wager](https://en.wikipedia.org/wiki/Pascal%27s_wager). If it's noble to sacrifice one's life to save others' lives, it must be nobler to sacrifice one's _after_-life to save others' afterlives. Assume 100% of babies go to heaven. Generously assume 90% of adults go to heaven. So if you kill 50 babies, you go to Hell, but 5 others who otherwise wouldn't go to heaven: Net +4 souls saved, and you'll be rewarded in the _after_-after-life!

[^separatism]: Common argument for white nationalism: Black/Latino crime rates are higher than whites', so "let's separate". Counter-argument: men's crime rates are also higher than women's, so that argument also implies women should secede from men (which is possible: we have IVF, and soon we'll have IVG, to make sperm from female cells). Counter-counter-argument: _YES._ Hence, White Nationalist Lesbian Separatism.

. . .

<a id="project_30"></a>

### 30. ⨁ Complex Jane: a love story with an imaginary friend

Jane is a mathematician with an imaginary friend, Ima. Together, they're *Complex Jane*: one part real, one part imaginary.

Get it? She's a mathematician, and it's a pun on *complex plane?*

![A photo of a slack-jawed moray, looking like it just told a bad pun.](/content/media/backlog/17064737174567.jpg)

Anyway, my main motivation for this project: **to write a chill, positive representation of plurality.**

Many asterisks aside, researchers agree: "the brain is a computer"[^scott-young-summary]. So: if a single computer-hardware can safely dual-boot multiple operating systems, why not a single brain-hardware safely hosting multiple personalities?

[^scott-young-summary]: Layperson-friendly summary of the scientific consensus, [by Scott Young (2023)](https://www.scotthyoung.com/blog/2023/07/25/the-mind-is-a-computer/)

Over the last few years, 4 of my friends (or should I say, 4+?) came out to me about having Dissociative Identity Disorder (DID) or some kind of plurality.

(If 4 sounds unlikely, keep in mind the prevalence of DID in the general population is ~1%[^DID-myths], the median person has about ~291 people in their personal network[^network-size], and I'm in LGBTQ communities where folks are ~1.66x more likely to have been physically/sexually abused as kids, a risk factor for DID.[^LGBTQ-ACE] [IMPORTANT NOTE: Not all "split personalities" are caused by PTSD, cPTSD, or trauma.] So, munching the numbers, \\( 0.01 * 291 * 1.66 \approx 4.83 \\), 4-or-5 folks I know having DID is what I'd statistically expect. As for the rest of you, \\( 0.01 * 291 = 2.91 \\), *you* can expect 2-or-3 folks you know to secretly have DID.)

[^DID-myths]: Brand et al (2016): [Separating Fact from Fiction: An Empirical Examination of Six Myths About Dissociative Identity Disorder](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959824/) See Table 1, "Dissociative Disorder Prevalence Studies > General Population". Results range from 1.10% (1 in 90) to 1.50% (1 in 67).

[^network-size]: McCarty et al (2005): [Comparing Two Methods for Estimating Network Size](https://meridian.allenpress.com/human-organization/article-abstract/60/1/28/71320/Comparing-Two-Methods-for-Estimating-Network-Size?redirectedFrom=fulltext) Aside: it turns out that "you know 150 people" (Dunbar's Number) [is kind of a myth](https://en.wikipedia.org/wiki/Dunbar%27s_number#Criticism).

[^LGBTQ-ACE]: Andersen & Blosnich (2013): [Disparities in Adverse Childhood Experiences among Sexual Minority and Heterosexual Adults: Results from a Multi-State Probability-Based Sample](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3553068/). Result: **Sexual minority persons had higher rates of adverse childhood experiences (IRR = 1.66 gay/lesbian; 1.58 bisexual) compared to their heterosexual peers.** (Note: Adverse Childhood Experience = "physical, sexual, and emotional abuse, neglect, exposure to domestic violence, familial mental illness, substance abuse and incarceration")

Having split personalities is still pretty darn stigmatized, as far as I can tell. *Every* mainstream representation of plurality that I can think of is horror (Psycho, Split, Red Dragon, Jekyll/Hyde), unhealthy (Fight Club, Mr. Robot), psychoanalytic BS (Sybil, Three Faces of Eve) or shock comedy (Me Myself & Irene, Flippy from Happy Tree Friends). I haven't seen United States of Tara, but it may be the *only* positive representation of plurality in popular media. (Calvin & Hobbes and Inside Out seem more "being a kid" and "internal family systems", respectively, than "plural".)

In contrast: in the last century, being left-handed, gay, bi, trans, or autistic (at least, the "minimal support needs"/formerly-Asperger's kind) has gone from "this is a illness to cure" to "this is a harmless, alternate way to be". (At least in my social circles.) I hope one day - both for my friends' sakes, and out of moral/intellectual principle - that plurality will be accepted, too.

Maybe with the help of storytelling, hence this project.

Also, I'd get to discuss the philosophy of math –– (how is math both *less* and *more* real than physical reality? with math & personhood/consciousness: what *is* real, anyway? should I even *care* what's "real"?) –– aaaaand I'd make math puns. For example, a well-meaning but un-informed therapist will recommend to Jane she "re-integrate" with Ima. That chapter will be titled... *Integration by Parts.*

![A photo of a slack-jawed moray, looking like it just told a bad pun.](/content/media/backlog/17064737174567.jpg)

**Progress:** 2% ◻️◻️◻️◻️◻️

I've done lots of informal interviews (aka "conversations") with plural-system friends. I have a collection of "vignettes" I want to tell, but not yet an overall frame/story to collect them in.

Maybe this'll be a Slice-of-Life story? After all, if the motivation is to show plurality as norm-acceptable, then making this story low-drama & wholesome may be the way to go. (Though that may risk making the story boring?)

**Time on backburner:** 3 years.

**Time to finish:** 2 weeks.

**Desire to finish:** 3/5. ⭐️⭐️⭐️☆☆

. . .

<a id="project_31"></a>

### 31. 👩‍👧 I've Become My Mother: a sci-fi story about raising yourself

![Diagram of frazzled mother & clone-daughter, with their minds being synchronized.](/content/media/backlog/mother.png)

There's a few problems with immortality.

1) Most proposals aren't possible with today's technology.
    * We can't yet reverse aging (and even if we could, you'd have no "backup body").
    * We can't simulate whole brain-scans (and even if we could, it may not be conscious, see ["dust is conscious" project pitch](#project_27)).
    * The current state of cryonics is dubious. Vitrification still damages neural tissue a lot, and even if future algorithms "fill in the gaps" for the missing brain-info, you may end up being a 99% generic-filled-in mind.
2) Even *when* immortality is possible, there'll be direct problems from that:
    * If society progresses "one funeral at a time", having people with 1000-year-old locked-in beliefs & values would make us stagnate.
    * Furthermore, without death, humanity's forced to make one of the following choices: a) live in a horrible overpopulated world, b) give up the joy of creating & raising children, or c) do expensive, dangerous, isolating space colonization.

But! What if I told you there was a method of immortality that *IS* possible with today's tech, *AND* gives you backup bodies & a consciousness-guarantee, *AND* lets us keep the joy of raising children, giving new youthful vitality to our species, *ALL WITHOUT* risking overpopulation or forced lonely space expansion?

You just need to... *expand* your definition of immortality.

First, let's define "you" to be your memories, personality, values, etc. This matches with our intuition of what makes you "you". Someone with severe Alzheimers "loses themselves" over time, even if they're in the same body. Someone who quickly freezes to death and then brought back to life, is still the same person, even if they legally & medically died. (Yes, there *are* cases like this.[^freezing-death])

[^freezing-death]: Clickbaity title, but correct as far as I can tell. Jen Babakhan (2022) for Reader's Digest: [6 People Who Froze to Death—and Came Back to Life](https://www.rd.com/list/people-who-froze-came-back-to-life/).

So, the get-immortal-quick scheme:

* Keep a journal throughout your life. (And retroactively record past memories.)
* Use AI to generate pictures, videos, even VR landscapes out of your journal, to make your memories vivid again. (Every time we remember something, we're reconstructing it in our head anyway.)
* Clone yourself. This has already been done with other primates. The main reason it hasn't been done yet with humans (*as far as we know*) is legal/political reasons.
* "Sideload" your memories into your kid-self, by having them watch your memories in vivid video/VR. It's as if they're recalling their past life. This also helps install your emotional-associations into them; what's good or bad, that is, *your* values.
* At the end of each day, *synchronize your memories* using the AI-generation-enhanced journal. You install your day's memories (condensed) into your kid-self, *and vice versa*.
* You could even synchronize your *in-the-moment* thoughts. Both of you wear a device to detect subvocalizations ([a real thing!](https://en.wikipedia.org/wiki/Subvocal_recognition)), then convert it into voice played into each others' earpieces. You could also synchronize audio-visual senses with a live video camera playing split-screen into each others' AR glasses.

And voilà! You have a new body that shares & synchronizes with your mind. Immortality, *and* the joy of raising a child who's also you.

*What's the worst that could possibly happen?*

Anyway, I have an idea for a sci-fi story spanning 100 years, about intergenerational trauma, and trying to be literally kinder to yourself.

**Progress:** 1% ◻️◻️◻️◻️◻️ Just the above high-concept.

**Time on backburner:** 3 years.

**Time to finish:** 2 weeks.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_32"></a>

### 32. 😻 Genetically Engineered Catgirl

The first 4 letters of "Catgirl" are

C
A
T
G

*The DNA alphabet!*

And "irl" is short for "In Real Life".

So: CATG irl = *Catgirl!!!*

...

That's it, that's the entire pitch.

**Progress:** 0% ◻️◻️◻️◻️◻️

**Time on backburner:** 3 years

**Time to finish:** ? ? ? There's nothing here

**Desire to finish:** 5/5 ⭐️⭐️⭐️⭐️⭐️

---

<a id="category_music"></a>

## 👩‍🎤 Songs / Music Videos

During the first year of the Covid-19 pandemic, I did [Andrew Huang's one-month course on producing songs in Ableton.](https://studio.com/classes/andrew-huang/music-production) I grew up on Weird Al & MC Frontalot, I just wanna make nerdy songs.

(I did publish *one* music video during the early pandemic. It was called, uh, [The Femboy Hooters Jingle](https://singingcatgirl.github.io/). ⚠️ Warning: very furry & very horny.)

**Turn on your speakers, the below project pitches have demos of my amateur singing! 🔊** I warn you, it's very, *very* amateur singing. It sucks. But as Jake the Dog once said:

![Jake the Dog saying: "Dude, sucking at something is the first step towards being sort of good at something."](/content/media/backlog/jake.gif)

33. The Special Relativity Song [↪](#project_33)
34. The General Relativity Song [↪](#project_34)
35. The Intrusive Thoughts Song [↪](#project_35)
36. The Pythagorean Theorem Song [↪](#project_36)
37. The Longest Term [↪](#project_37)
38. The 3Blue1Brown Jingle [↪](#project_38)
39. Brand New Fetish [↪](#project_39)

. . .

<a id="project_33"></a>

### 33. 🚀 The Special Relativity Song

This was my final project for Andrew Huang's course! I warn you, it has a very "she learnt music production & singing in one month" quality:

[Full song here](https://studio.com/p/TKajnA)

Excerpt:

<audio controls src="https://ncase.me/misc-dump/2024-02/sucky-singing/SR.mp3"></audio>

Lyrics Excerpt:

> I'm float / ing on a boat    
> At a constant velocity    
> And you're / on the shore    
> Just standing there observing me    
>
> To you I'm moving at a speed of `v`    
> But you move at minus `v` according to me    
> That's all / what we call    
> Galilean relativity!    
>
> `[...]`

**Progress:** 60% ▶️▶️▶️◻️◻️ Lyrics & melody complete. Production done but it sucks so I want to re-do it, or hire a much more musically & vocally-skilled friend. Music video animatic'd in my head.

**Time on backburner:** 2 years.

**Time to finish:** 4 weeks (with animation)

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_34"></a>

### 34. 🏀 The General Relativity Song

If the above song & music video took off, I was thinking of making a sequel.

Demo:

<audio controls src="https://ncase.me/misc-dump/2024-02/sucky-singing/GR.mp3"></audio>

Lyrics (excerpt):

> [Chorus]    
> Einstein once set out to prove    
> Why an apple does fall down to Earth    
> Curved spacetime tells mass how to move    
> and mass tells spacetime how to curve!    
>
> [Verse 1]    
> *My happiest thought,* Einstein tells the truth,    
> *was imagining a man falling off a roof.*    
> Schadenfreude? Au contraire!    
> Coz when that man falls through the air,    
> Ignore the wind in his hair and on his face,    
> He'll feel weightless -- like in outer space    
> Indistinguishable from an inertial frame    
> Principle of Equivalence: it's the same!    
> But wait! A question on your mind:    
> Inertial things move in straight lines    
> But things fall in curves, so how can it be straight?    
> Well very simply, if spacetime is -- *wait!*    
> -- *for it!* --    
> . . .    
> Curved *itself.* Yeaaaaaaaah!!!!!!    
>
> `[...]`

**Progress:** 20% ▶️◻️◻️◻️◻️ Lyrics & melody.

**Time on backburner:** 2 years.

**Time to finish:** 6 weeks. (with animation)

**Desire to finish:** 1/5 ⭐️☆☆☆☆ (Eh, [Veritasium's video on General Relativity is already really good.](https://www.youtube.com/watch?v=XRr1kaXKBsU) My thing would be redundant.)

. . .

<a id="project_35"></a>

### 35. 🙀 The Intrusive Thoughts Song

Intrusive thoughts are something I (and a lot of people with anxiety disorder/OCD) struggle with.

However, most public health resources on intrusive thoughts keep their writing tame, like, "oh it's when you imagine yourself jumping off a bridge or robbing a bank", which (unintentionally) makes anyone else with *worse* thoughts feel, "oh my god I'm a freak even by *freak* standards".

So, I wanted to make a song about intrusive thoughts that's *far* grosser - but with comedy, to help defuse the anxiety that comes with it!

**(⚠️ WARNING: *EXTREMELY* NSFW LYRICS. CONTENT NOTE: EVERYTHING ⚠️)**

Demo:

<audio controls src="https://ncase.me/misc-dump/2024-02/sucky-singing/intrusive.mp3"></audio>

Lyrics (excerpt):

> Don't think / of a pink / elephant    
> Sucking its own dick with its own trunk    
> I guess / when you repress / your sentiments    
> Your mind gets filled-up with fucked-up junk    
>    
> We all got! [clap clap] Intrusive thoughts! [clap clap]    
> Your mood may worsen / Thinking "I'm a bad person"    
> But you're not! [clap clap] Intrusive tho-o-o-oughts!    
> Are something we all share, so sigh a deep breath    
> We all fantasize about fucking dogs to death!    
>    
> ooooooohhhhhhhhhhhhhhhhh    
>    
> This isn't your first rodeo    
> You know that beheading video    
> That you saw when you were ten?    
> It'll pop up in your mind again    
> and again and again and again and again    
> and -- what rhymes with ten? -- *again!*    
> [clap clap]    
>    
> “Let's shoot up a school!”    
> “Let's jerk off to gore!”    
> “Let's shout the N-word in a grocery store!”    
> What's with this melodrama?    
> Is it my childhood trauma?    
> I'm getting *pissed.*    
> [clap clap]    
> Why the cognitive-behavioral *fuck*    
> does this part of my brain exi-i-i-iiiiiiiiiiiiist?    
>    
> `[...]`    

**Progress:** 20% ▶️◻️◻️◻️◻️ Lyrics & melody.

**Time on backburner:** 1 year.

**Time to finish:** 3 weeks for just the song, 8 weeks with animation.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_36"></a>

### 36. 📐 The Pythagorean Theorem Song

This one *requires* a music video, since it's about a visual proof of the Pythagorean Theorem.

Demo:

<audio controls src="https://ncase.me/misc-dump/2024-02/sucky-singing/triangle.mp3"></audio>

Lyrics (excerpt):

> So you know two cities' longitude & latitude    
> Can we get their distance? With a can-do attitude!    
> and a right triangle -- no need to fear 'em    
> I'll show you the picture proof of the Pythagorean Theor-e-e-e-em!    
>
> So you have a triangle    
> That has a right angle    
> `a` and `b` are the sides    
> and `c`'s the diagonal    
> `[...]`    

**Progress:** 20% ▶️◻️◻️◻️◻️ Lyrics & melody. Music video animatic'd in my head. No production.

**Time on backburner:** 2 years.

**Time to finish:** 4 weeks for song & animation

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_37"></a>

### 37. 🧐 The Longest Term

A parody of Billy Joel's _The Longest Time_ ([on YouTube](https://www.youtube.com/watch?v=a_XgQhMPeEQ)), but it's a friendly ribbing of the rationalist/longtermist movement. (This came out of a late-night hangout with some teens at [Hack Club](https://hackclub.com/).)

Demo:

<audio controls src="https://ncase.me/misc-dump/2024-02/sucky-singing/longest.mp3"></audio>

Lyrics (excerpt):

> If you said that happiness was good    
> And you'd like to make more if you could    
> Then you should put 'yer    
> Moral weight on folks in the future    
> Coz they outnumber us in the looooongest term!    
>
> Build a Dyson Sphere around the Sun    
> To power the Matrix for everyone    
> All eight quintillion    
> Quintillion quintillion quintillion    
> Happy brain-em's in the looooongest term!    
>
> [...]

**Progress:** 20% ▶️◻️◻️◻️◻️ Lyrics & melody.

**Time on backburner:** 2 months.

**Time to finish:** 3 weeks for just the song, 6 weeks with animation.

**Desire to finish:** 1/5 ⭐️☆☆☆☆

. . .

<a id="project_38"></a>

### 38. 🥧 The 3Blue1Brown Jingle

A birthday present for Grant Sanderson I never finished.

Demo:

<audio controls src="https://ncase.me/misc-dump/2024-02/sucky-singing/math.mp3"></audio>

Lyrics (excerpt):

> Have you been wishin'    
> For visual intuition    
> of math?    
>
> Because mindless plugging    
> and chugging is bugging    
> me, and fills me with wraaAAAAATH    
>
> `[death metal]`    
> BUT DON'T FEEL DOWN    
> THREE BLUE ONE BROWN    
>
> --will show you the beauty of math!    

**Progress:** 20% ▶️◻️◻️◻️◻️ Lyrics & melody.

**Time on backburner:** 3 years.

**Time to finish:** 1 week for just the song, 2 weeks with an animation.

**Desire to finish:** 0/5 ☆☆☆☆☆ sorry Grant

. . .

<a id="project_39"></a>

### 39. 🐺 Brand New Fetish

This was meant to be a follow-up to *The Femboy Hooter Jingle* if it was successful, and if I wanted to keep making musical furry erotica as a side-hustle.

Right, not sure if I mentioned that. I tried running a lewd furry Patreon during the pandemic. Didn't pan out. Anyway...

**(⚠️ WARNING: NSFW LYRICS)**

Demo:

<audio controls src="https://ncase.me/misc-dump/2024-02/sucky-singing/fetish.mp3"></audio>

Lyrics (excerpt):

> So you're browsing on the internet    
> When you find some porn that makes you hard or wet    
> It's sickening and gross and awful, yet    
> I think you got yourself a new fetish!    
>
> Red Riding Hood can ride a werewolf knot    
> That incest porn has a surprisingly good plot    
> And Vaporeon is a moist Poké-thot    
> What hath God wrought?    
> I think you've got    
> a braaaaand newwww fetish!    
>
> `[...]`

**Progress:** 20% ▶️◻️◻️◻️◻️ Lyrics & melody.

**Time on backburner:** 3 years.

**Time to finish:** 3 weeks for just the song, 12 weeks with the raunchy animation.

**Desire to finish:** 1/5 ⭐️☆☆☆☆

---

<a id="category_misc"></a>

## 🤷🏻‍♀️ Miscellaneous

And finally... the leftovers:

40. Trans voice tools/tutorials [↪](#project_40)
41. Tool for making drip-feed courses (& courses I want to make) [↪](#project_41)
42. (Something on Georgist Economics) [↪](#project_42)
43. (Something on Pandemic Preparedness) [↪](#project_43)
44. The Exit Strategy [↪](#project_44)
45. We do advocacy, they do propaganda: A Collection of Russell Conjugations [↪](#project_45)
46. Common Wisdom is a Two-Timing Sumnabitch [↪](#project_46)
47. Clearing out the idea closet (47 projects on my backlog) [↪](#project_47)

. . .

<a id="project_40"></a>

### 40. 🧑‍🎤 Trans voice tools/tutorials

Scarlett Johansson and Slim Shady's voice has around the same pitch.

So why does Scarlett sound femme while Slim sounds masc? Because voice gender isn't *just* about pitch: it's also (at least) about resonance.

Alas, all the (publicly available) trans voice apps focus entirely on *pitch*. So, for my own sake, and for other trans Friends, I tried making a *spectrogram*-based voice training app. A spectrogram does fancy math (Fourier Transform) on audio to get not just its pitch, but *also* its resonance, formants, turbulence, open quotient (OQ), and more!

But wait, there's more! All voice apps also require you *look* at the app for feedback. My idea: if you have earphones with a microphone attached, the app would play feedback cues into your ear *while you spoke in the real world*. (e.g. if your pitch is too high, it plays a declining pitch)

Silent GIF of my prototype from four years ago:

![GIF of a spectrogram, with visual markers for target pitch & resonance.](/content/media/backlog/VoiceApp.gif)

Aaaaaaaand I've been stalled ever since. It turns out doing Digital Signal Processing, let alone in *JavaScript*, is hard. Also, my current main struggle with my voice is *physical strain*, even when *I'm not speaking at all*. I have bad "throat posture". This app wouldn't help with that.

I *may* pick up this project again in the future, but honestly don't hold your breath. Other trans-voice related project ideas:

* A "general audience" science video on trans voice. (The *physics* of voice itself is fascinating! And spectrograms use *math!*)
* A web toy that modifies a voice from "masc" to "femme" and vice versa. Because: 1) It's cool, 2) It's proof/demonstration that voice gender is "just" pitch+resonance+OQ, and 3) It lets you preview your "transitioned" voice, to get euphoria vibes, or test if you're an egg that hasn't hatched yet. <3
* A web toy that lets you synthesize voices in the browser. Silent GIF of a prototype below:

![GIF of a prototype voice synthesizer](/content/media/backlog/VoiceSynth.gif)

**Progress:** 40% ▶️▶️◻️◻️◻️ Already made a working web-app spectrogram, as you can see above. But I made that 4 years ago & got stuck.

**Time on backburner:** 4 years. 😭

**Time to finish:**

4 weeks? (for the biofeedback tool)    
2 months? (for a general-science video)    
?!?!? (for that insane voice-genderswap software)    
1 month? (for the voice-synth)

**Desire to finish:**

2/5 ⭐️⭐️☆☆☆ (for the biofeedback tool)    
1/5 ⭐️☆☆☆☆ (for all the others)

. . .

<a id="project_41"></a>

### 41. 💧 Tool for making drip-feed courses (& courses I want to make)

You know how sometimes you sign up for a service, they'll email you on Day 1, then Day 2, then Day 3 etc with pre-written "onboarding" emails? These are called "drip marketing" newsletters.

I didn't realize until recently you could use them to give *online courses!* For example, I recently took Ali Abdaal's free 30 Day Productivity email-course[^ali-abdaal] – (did I mention I shipped *nothing* in the last 14 months?) – and I *adored* it.

[^ali-abdaal]: Alas, the course already ended, but [here was the website](https://30dayproductivity.com/). Bafflingly, there's no official archive of the emails he sent out.

I make educational stuff online, but I've felt "eh" that all my explainers are one-shot standalones. I want to try making a *course*.  I *could* do a drip-marketing newsletter, but:

1) That gets expensive for *me* very quickly
2) Email clutter sucks
3) Folks are *correctly* wary of putting their emails into things.

So I thought: why not use the JavaScript library [ics.js](https://github.com/nwcell/ics.js/) to *automatically generate* a bunch of .ics calendar events, and each event would have a link to the thing you're supposed to read/do? Bonus: unlike the drip-marketing email, the learner can even schedule and block out *when in the day* they'd like to do the course!

Ideas for courses I wanna make:

* Convert 3blue1brown's "Essence of Calculus/Linear Algebra" video series (with permission) into a course with practice problems & Anki flashcards to download.
* A course on game theory, since that's the topic I'm "most famous" for.
* A course on public choice theory (& its limitations). Public choice theory is game theory applied to politics.
* A course on learning HTML/CSS/JS, where you get to "take back the web"; make your own website, use extensions to modify the CSS or JS of webpages, etc.
* Something about designing systems/organizations that are robust & anti-fragile. All I know is I want to use the title: "Intelligently Designing for Evolution".
* A course on educational writing, to go meta. Teaching folks how to teach folks.

**Progress:** 0% ◻️◻️◻️◻️◻️

**Time on backburner:** Had the idea for the tool 1 week ago, had the desire to make the above courses for 5 years.

**Time to finish:** 1 week for the tool, 1 to 4 months for the courses.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_42"></a>

### 42. 💸 (Something about Georgist Economics)

I make educational games. One of the first-ever edu-games was *The Landlord's Game*, a board game by Elizabeth Magie, a Quaker and believer in Georgist Economics. *The Landlord's Game* was meant to show the flaw in letting people to privately collect rent from land, and show a solution: a land-funded basic income.

Magie's board game was swiped without credit, and sold by Parker Brothers as *Monopoly.* The "basic income" thing became "pass Go and collect $200".

Anyway, that's why I've *heard* of Georgist economics before, but it wasn't until I read [Lars Doucet's explainer of Georgism](https://www.astralcodexten.com/p/does-georgism-work-is-land-really) that I thought, *wow this is actually a really powerful idea!*

Of course, the field of economics has developed a *lot* since 1879, so I'd like to re-explain Georgism from a modern microeconomic viewpoint. This means I'll have to make simulations to explain microeconomics in *general*, then explain why Georgism logically falls out of that.

(There's also been recent extension of Georgist ideas to dealing with automation/AI — certainly something that's on everyone's minds *now*.)

**Progress:** 15% ▶️◻️◻️◻️◻️ So far, I've made a basic supply-and-demand simulator for microeconomics:

![A GIF of an interactive version of the classic Supply & Demand diagram](/content/media/backlog/SupplyDemand.gif)

No Georgism-specific stuff yet, though.

**Time on backburner:** 3 years

**Time to finish:** 2 months if "just" an article, 3 months if I expand it into a video essay.

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆

. . .

<a id="project_43"></a>

### 43. 🦠 (Something about Pandemic Preparedness)

Like most folks, I got really sad by how badly humanity handled the Covid-19 pandemic. Fuck, I mean,

`[1,000 word rant redacted]`

So, what's a path for pandemic preparation (both "natural" and bio-terrorist) that is:

1) Something even *a single town* can do for themselves, since apparently humanity can't coordinate for shit.
2) Something that requires *as little human willpower* as possible, and
3) Mostly targets *aerosol transmission*, since that's the quickest way to infect & kill a lot of people.

A family of solutions that fits all three: air filters, ventilation, germicidal UV, far UV-C, etc. A building can install those, and the humans inside wouldn't have to change any behavior whatsoever.

Project Pitch: I dunno, *some* kind of video/article/advocacy piece for better research into that. Maybe some interactive explanations of aerosol transmission models. And/or present an *honest* economic case for this. (e.g. bio-terrorism & novel pandemics aside, even the "regular" flu costs the U.S. $90 billion a year.[^cost-flu])

[^cost-flu]: Dobson (2007): [Flu costs the US $90bn a year, report shows](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1885332/)

**Progress:** 0% ◻️◻️◻️◻️◻️ Nothing but a project pitch & guilt.

**Time on backburner:** 4 years

**Time to finish:** 2 months

**Desire to finish:** 1/5 ⭐️☆☆☆☆ (Not sure why my desire to do this is so low, given that *intellectually* I know it's an important topic. Maybe I don't think *I'm* the right person for it, or the topic is so depressing I just reflexively shy away.)

. . .

<a id="project_44"></a>

### 44. 🖕 The Exit Strategy

![Meme image of a skeleton saying "JUST WALK OUT" and "IF IT SUCKS... HIT DA BRICKS!!"](/content/media/backlog/17068148068693.jpg)

The year 2020 got me blackpilled for a while, not going to lie. Most major institutions simultaneously losing credibility, *and* the "anti-establishment" folks being about as irrational. (If institutions/the public are as unreliable as a coin flip, you saying Heads when they say Tails and vice versa makes you *also* as bad as a coin flip.)

So, I felt bummed for a while. The forces that control the world are criminally incompetent *at best*, and I'm too small to do anything about it. I step in front of the tank in protest, I'll just get squashed. What to do?

Eventually, a few things started giving me hope:

* Folks making home-made masks while the WHO & CDC were still recommending *against* masks. (The institutions later said, "oh we just told the public to not wear masks because we wanted to conserve supplies for hospitals". Lest it be memory-holed: *nurses at hospitals* were being fired for wearing N95 masks[^nurse-masks])
* The creation & dissemination of [the Corsi-Rosenthal box](https://en.wikipedia.org/wiki/Corsi%E2%80%93Rosenthal_Box), a relatively-cheap-but-efficient air filter you can make at home!
* An acquaintance of mine, a trans woman in biotech, making her own high-quality estrogen, and giving it away for free/pay-what-you-can. (A bottle costs $10 in materials; it usually costs $200 out-of-pocket.)
* [Sci-Hub](https://en.wikipedia.org/wiki/Sci-Hub), a science-paper piracy website, so the public can access science which was *already* funded with public taxpayer money. (When you buy a science paper, that money goes to the journal, *not* the scientists. It doesn't even go to the *peer reviewers*; they're unpaid.)
* Slime Mold Time Mold's alternate-universe non(?)fiction piece, ["Effective Illegalism"](https://slimemoldtimemold.com/2022/08/04/effective-illegalism/), which lists lots of other sneaky ways to get around unjust laws: Biohackers making their own insulin, Marrying foreigners to give them green cards, etc.

[^nurse-masks]: Example 1 [(Mar 24, 2020)](https://chicago.suntimes.com/coronavirus/2020/3/24/21192690/northwestern-memorial-hospital-sued-better-face-masks-lauri-mazurkiewicz). Example 2 [(April 16, 2020)](https://6abc.com/coronavirus-pandemic-covid-19-hospital-workers-health-care/6106053/). Akela Lacy (2020) for The Intercept: [Kaiser Permanente Threatened to Fire Nurses Treating Covid-19 Patients for Wearing Their Own Masks, Unions Say](https://theintercept.com/2020/03/24/kaiser-permanente-nurses-coronavirus/)

Is this just the old debate of "working to change the system from the inside" versus "working to change the system from the outside"? Well, yes. But a few things to emphasize:

1. It's not either/or, you can try to change the system from the inside *and* outside. (You can have an Underground Railroad *and* try to get slavery abolished.)
2. Trying to "change a system from the inside" carries the risk of moral corruption: the system changing *you* from the inside.
3. *Building alternatives & exit plans gives you more bargaining power to change systems from the inside.* Like how having multiple job offers lets you negotiate a higher salary, having the people in power know *you do not need to depend on them* lets your voice have more negotiation power. (You can show this formally in game theory: see [the Exit, Voice, Loyalty model](https://en.wikipedia.org/wiki/Exit,_Voice,_and_Loyalty_Model))

Project Pitch: I'd like to make an article sharing lots of examples + spread the mindset of direct-action / self-efficacy / a willingness to commit pro-social crimes like pirating insulin.

**Progress:** 20% ▶️◻️◻️◻️◻️

**Time on backburner:** 2 years

**Time to finish:** 2 weeks

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆ Slime Mold Time Mold's post is already pretty great, so I'm not sure what value-add I can do. Maybe focus more on the game theory?

. . .

<a id="project_45"></a>

### 45. ⚖️ We do advocacy, they do propaganda: A Collection of Russell Conjugations

![Symmetric picture of two warring kingdoms. On the left: "Our blessed homeland", "our glorious leader". On the right: "Their barbarous wastes", "their wicked despot". The punchline is the picture's completely symmetrical, save for different-colored flags.](/content/media/backlog/17068911370429.jpg)*("Our Blessed Homeland" (2015) by Tom Gauld. [Source](https://twitter.com/tomgauld/status/571994690289061888))*

> [Having a set of strongly-held beliefs]
> *We* stick to our principles.
> *They* have a dogmatic ideology.
>
> [Persuasive messaging]
> *We're* doing advocacy, speaking to the heart.
> *They're* doing propaganda, manipulating emotions.
>
> [Doing something small but nice]
> *We're* raising awareness, showing solidarity, getting a small win.
> *They're* virtue signaling, doing it for PR, they're [x]-washing.
>
> [An idea that's popular]
> *Our* idea is common sense, self-evident.
> *Their* idea is fashionable, a trite truism.
>
> [An idea that's unpopular]
> *Our* idea is shocking, it challenges the status quo.
> *Their* idea is cringe, they're a bunch of edgelords.

And so on.

> [Neutral description]
> *We* [positive spin]
> *They* [negative spin]

You've heard it over and over; critical thinking requires weighing arguments & evidence even-handedly, every idea deserves its fair trial in court, yada yada.

Alas, most of us, most of the time, think in *words*.  And, like the proverbial glass that's half-full or half-empty, you can always describe *the exact same thing* in a way that sounds noble or like shit.

(These are called "emotive conjugations" or "Russell Conjugations", named after the mathematician-philosopher Bertrand Russell.)

After some... *very* infuriating conversations with some policymakers, and realizing *my god it's full of wordcels*... I just want to make a big frickin' list of Russell's Conjugations, so that:

1) You know it's a warning sign when someone dismisses ideas as "propaganda" or "cringe" or "fashionable" without *actually weighing the pros/cons of an idea*.
2) You can catch yourself & avoid using these conjugations in your *own* head, to make yourself a better critical thinker.

**Progress:** 60% ▶️▶️▶️◻️◻️ Honestly, the above writing is most of it.

**Time on backburner:** 3 years

**Time to finish:** A few days

**Desire to finish:** 4/5 ⭐️⭐️⭐️⭐️☆ Strangely, even though the *idea* of the Russell Conjugation is pretty well-known, I can't find a curated list of them online. So, nice value-add.

. . .

<a id="project_46"></a>

### 46. 💬 Common Wisdom is a Two-Timing Sumnabitch

Here's another post cautioning against being a wordcel who "thinks in words, not things".[^holmes] Common wisdom, in the form of proverbs, is a two-timing sumnabitch, because you can almost always find a proverb arguing the *opposite* idea:

[^holmes]: Riff off a quote by Oliver Wendell Holmes Jr. [From Holmes (1899)](https://www.jstor.org/stable/1321177): “**We must think things not words**, or *at least* we must constantly translate our words into the facts for which they stand, if we are to keep to the real and the true. [emphasis added]”

> "Birds of a feather flock together."
> "Opposites attract."
>
> "Take a leap of faith."
> "Look before your leap."
>
> "Don't sweat the small stuff."
> "For want of a nail [...] the kingdom was lost."
>
> "If at first you don't succeed, try try again."
> "The definition of insanity is doing the same thing over and over and expecting different results."
>
> "The road to hell is paved with good intentions."
> "It's the thought that counts."

You can even find "common wisdom" arguing *for and against* "common wisdom":

> "When in Rome, do what the Romans do."
> "If all your friends jumped off a bridge, would you too?"

This article was inspired by me getting mad at a movie review dismissing a movie I liked for having a trite message that could be summed up in a proverb. Bitch, *every statement and its negation already has a proverb*. The existence of a proverb is *zero* evidence for or against something's truth.

**Progress:** 60% ▶️▶️▶️◻️◻️

**Time on backburner:** 3 years

**Time to finish:** A few days

**Desire to finish:** 2/5 ⭐️⭐️☆☆☆ There's already many lists online that collect contradictory common sayings. My only "value-add" is framing it around a cautionary message: *Don't be a wordcel. Go beyond words, and look at the actual damn thing.*

(...she said, after typing 20,000+ words about all the things she *didn't* ship...)

. . .

<a id="project_47"></a>

### 47. 🧹 Clearing out the idea closet (47 projects on my backlog)

![Nicky-Catgirl dead-exhausted from sweeping. Dead on the floor making a thumbs-up hand gesture.](/content/media/backlog/footer.png)


THIS CLOSET... IS CLEAN!

This exercise was... not worth it, but I learnt some things!

* Apparently, I can write 20,000 words *in one week* if I turn off my perfectionist-anxiety filter.
* And now that I've reviewed my whole backlog, I've a better sense of what I'd like to prioritize (anything 3 ⭐️ or above), and what I'd drop (anything less than 2 ⭐️). I now know *what I actually value.*

I'd like to give you a better ending, but again: goodbye perfectionist-filter. No polish here. The moral of the story is there is no moral and there is no story.

I'm just *clearing out the closet.*

**Progress: 100%** ▶️▶️▶️▶️▶️ I'M DONE

**Time on backburner:** 2 years

**Time _it took_ to finish:** 1 week

**Desire to finish:** 5/5 ⭐️⭐️⭐️⭐️⭐️ I'M *DONE!*

---

<iframe src="https://ncase.me/ncase-credits/signup.html" frameborder="no" width=640 height=200></iframe>

<iframe src="https://ncase.me/ncase-credits/supporters/feb2024.html" frameborder="no" width=640 height=640></iframe>

## : Adiabatic Quantum Computation

*(the below was a text I sent to a friend, slightly edited)*

So: adiabatic quantum computation (AQC)!

I kinda visualize the core idea like, I start with a simple valley with a bunch of magic water in it at its global minimum. The magic water can, uh, tunnel through hills I guess.

Then, I slowly morph that landscape into a landscape whose global minimum "describes" the solution to some problem I want. (e.g. each position in the landscape describes a path for a travelling salesman problem, the height is how long the path takes, the global minimum is what I want).

If I morph the hills *slowly* enough, the magic tunneling-water will *stay* in the global minimum throughout the evolution. It won't get trapped because it's magic and can tunnel through newly-appearing hills; again, *as long as the evolution is slow enough*.

Tying metaphor to AQC: You start with a simple Hamiltonian in its ground state. Then, *slowly* evolve the Hamiltonian to one that describes a problem you care about, and whose global minimum is the solution you want. If the evolution is slow enough, your wavefunction will stay in the ground state throughout!

This isn't just empirical, it's been mathematically proved from the rules of quantum mechanics, the [Adiabatic Theorem](https://en.wikipedia.org/wiki/Adiabatic_theorem). (I mean, *I* can't follow the proof, but good to know it exists).

The catch, you may figure, is in the phrase "slowly enough".  HOW slow does it have to be?  [Wikipedia claims](https://en.wikipedia.org/wiki/Adiabatic_quantum_computation) that the runtime is proportional to the *inverse square* of the gap between the global minimum and the next-highest local minimum. 1/g^2, where 'g' is the gap.

But then, why have AQC / quantum annealing computers like D-Wave not had huge successes? Because the way folks convert problems like Traveling Salesman into landscapes, leads to local-global minima gaps that get _exponentially_ small as the problem size increases.

But my quantum octopus/fuzzy logic idea *doesn't* have this problem! The energy landscape my fuzzy gates create don't *have* false local minima, just the one saddlepoint!  And even if that saddlepoint counts as a local minimum, the gap between it and the global minimum *remains constant*.  (The saddlepoint is always at y = 0.5, the global minimum is at y = 1.0) Therefore, as far as I can tell, that means *in theory* (practical decoherence aside) AQC should be able to solve my quantum octopus in constant time.

Anyway "big if true", I've no idea, guess I'll faff around and find out.